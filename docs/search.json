[
  {
    "objectID": "Projects/IntDataAnalysis/Earthquakes/Earthquake.html",
    "href": "Projects/IntDataAnalysis/Earthquakes/Earthquake.html",
    "title": "Earthquake Relationships",
    "section": "",
    "text": "This project analyzes earthquake mainshocks and their aftershocks using a dataset collected from the Atacama Fault Zone.\n\n\nShow code\nlibrary(readr)\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nShow code\nmidterm1 &lt;- read_csv(\"midterm1.csv\")\n\n\nRows: 110 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): aftershock, mainshock\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nShow code\n# residual diagnositics with no transformation\nx= midterm1$mainshock\ny= midterm1$aftershock\n\nmodel = lm(y~x)\nplot(x,y)\nabline(lm(y~x))\n\n\n\n\n\n\n\n\n\nShow code\npar(mfrow=c(2,2))\nplot(model)\n\n\n\n\n\n\n\n\n\nlam is 0.73\n\n\nShow code\n#transforming y\ntrans.y = function(lambda){\n    gy = (y^lambda - 1)/lambda\n    SSR = sum((lm(x~gy)$residuals)^2)\n    SSR\n}\n\nlam = optim(1,trans.y)$par\n\n\nWarning in optim(1, trans.y): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\nShow code\ngy = (y^lam)/lam\nplot(x,gy)\nabline(lm(gy~x))\n\n\n\n\n\n\n\n\n\nShow code\nlam\n\n\n[1] 0.7261719\n\n\nShow code\nmodel1 = lm(y~x)\npar(mfrow=c(2,2))\nplot(model1)\n\n\n\n\n\n\n\n\n\nShow code\nmodel2 = lm(gy~x)\npar(mfrow=c(2,2))\nplot(model2)\n\n\n\n\n\n\n\n\n\nSTEP A\nThe intercept is .42: predicts a slight positive aftershock after mainshock mag is 0\nSlope is .96: for every 1 mag increase in mainshock the aftershock inscrease by .96 Showing a strong pos relationship.\nP-val is 2.2e-16 meaning the positive relationship isnt something random\nR^2: 86%. very strong. showing aftershock mag are can heavily be predicted by the mainshock mag\nResiduals show that linear model is reasonable and no transformation is needed.\nIE: bigger earthquakes have bigger aftershocks\n\n\nShow code\n# Load data\nx &lt;- midterm1$mainshock\ny &lt;- midterm1$aftershock\n#transforming y\ntrans.y = function(lambda){\n    gy = (y^lambda - 1)/lambda\n    SSR = sum((lm(x~gy)$residuals)^2)\n    SSR\n}\n\nlam = optim(1,trans.y)$par\n\n\nWarning in optim(1, trans.y): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\nShow code\ngy &lt;- (y^lam - 1) / lam\nmodel_bc &lt;- lm(gy ~ x)\nsummary(model_bc)\n\n\n\nCall:\nlm(formula = gy ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.52350 -0.17697  0.00849  0.21145  0.68690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.96201    0.11021  -8.729 3.55e-14 ***\nx            0.96168    0.03657  26.297  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.258 on 108 degrees of freedom\nMultiple R-squared:  0.8649,    Adjusted R-squared:  0.8637 \nF-statistic: 691.5 on 1 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nShow code\npar(mfrow=c(2,2))\nplot(model_bc)\n\n\n\n\n\n\n\n\n\n\n\nShow code\nlibrary(ggplot2)\nggplot(midterm1, aes(x = mainshock, y = aftershock)) +\ngeom_point() +\ngeom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\ntheme_minimal(base_size = 14) +\nlabs(x = \"Mainshock Magnitude\", y = \"Aftershock Magnitude\",\ntitle = \"Initial Relationship Between Mainshock and Aftershock\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nScatterplot of Mainshock vs Aftershock Magnitudes\n\n\n\n\nPART B\nmainshock of magnitude 5.0 is expected to be followed by an aftershock of around 6.27 magnitude, with 95% confidence that the true mean aftershock lies between 6.01 and 6.53.\n\nThis supports the idea that as mainshock magnitude increases, the aftershock magnitude also tends to rise in a predictable, nearly linear way.\n\n\nShow code\npred_bc &lt;- predict(model_bc,\n                   newdata = data.frame(x = 5),\n                   interval = \"confidence\")\n# Back-transform\nyhat_conf &lt;- (lam * pred_bc + 1)^(1/lam)\nyhat_conf\n\n\n       fit      lwr      upr\n1 6.270939 6.012455 6.532373\n\n\nPART C\nFor a mainshock of magnitude 5.0, we are 95% confident that the next aftershock will fall between 5.45 and 7.07 in magnitude.\n\nThis indicates that while a 6.0 aftershock is quite plausible, magnitudes as low as 5.4 or as high as 7.0 could also reasonably occur, given the natural variability in the data.\n\n\nShow code\nset.seed(1254)\nn &lt;- length(x)\norig_res &lt;- model_bc$residuals\nBS.ystar &lt;- numeric(10000)\n\nfor (i in 1:10000) {\n  BS.x &lt;- sample(x, n, replace = TRUE)\n  BS.res &lt;- sample(orig_res, n, replace = TRUE)\n  BS.gyhat &lt;- predict(model_bc, newdata = data.frame(x = BS.x))\n  BS.gy &lt;- BS.gyhat + BS.res\n  BS.model &lt;- lm(BS.gy ~ BS.x)\n  BS.ystar[i] &lt;- predict(BS.model, newdata = data.frame(BS.x = 5)) +\n                 sample(orig_res, 1)\n}\n\nlow.bound &lt;- sort(BS.ystar)[250]\nhigh.bound &lt;- sort(BS.ystar)[9750]\n\n# Back-transform\nlow.bound &lt;- (lam * low.bound + 1)^(1/lam)\nhigh.bound &lt;- (lam * high.bound + 1)^(1/lam)\n\nlow.bound\n\n\n[1] 5.450253\n\n\nShow code\nhigh.bound\n\n\n[1] 7.073021\n\n\nShow code\nmean(exp(BS.ystar) &gt; 6)\n\n\n[1] 1\n\n\n\n\nShow code\nhist(exp(BS.ystar),\nbreaks = 30, col = \"#56B4E9\",\nmain = \"Bootstrap Distribution of Predicted Aftershocks (x = 5)\",\nxlab = \"Aftershock Magnitude (Exponentiated)\")\nabline(v = exp(c(low.bound, high.bound)), col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\nBootstrapped Prediction Interval for x = 5"
  },
  {
    "objectID": "Projects/IntDataAnalysis/CarPrices/Cars4Sale.html",
    "href": "Projects/IntDataAnalysis/CarPrices/Cars4Sale.html",
    "title": "Helping Frank & Tron Price Cars (Without Crashing the Dealership)",
    "section": "",
    "text": "Show code\nlibrary(readr)\nShow code\n#read in data\nCars &lt;- read_csv(\"Cars.csv\")\n\n\nRows: 234 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Vehicle Name\ndbl (8): Hybrid, SuggestedRetailPrice, EngineSize, Cylinders, Horsepower, Hi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nShow code\n#making linear model based on ALL the variables\nmodel &lt;- lm(SuggestedRetailPrice ~ \n            Hybrid + EngineSize + Cylinders + Horsepower + HighwayMPG + Weight + WheelBase, \n            data = Cars)\n#remvoing outlier\n  #removed because it was only electric so it was expensive\n  #only engine with 3 cylinders\nCars = Cars[-67,] \n#renaming got it from Ashley\nx1 = Cars$Hybrid\nx2 = Cars$EngineSize\nx3 = Cars$Cylinders\nx4 = Cars$Horsepower\nx5 = Cars$HighwayMPG\nx6 = Cars$Weight\nx7 = Cars$WheelBase\ny = Cars$SuggestedRetailPrice\n\nX = data.frame(x1,x2,x3,x4,x5,x6,x7)\nY = y\nYX = data.frame(y,x1,x2,x3,x4,x5,x6,x7)\n\npower.trans = function(lambda){\n  Y.trans = (Y^lambda - 1)/(lambda)\n  Y.hat = lm(YX)$fit    \n  RSS.lambda = sum(lm(Y.hat~Y.trans)$residuals^2)\n  RSS.lambda\n}\n\n\noptim(1,power.trans)\n\n\nWarning in optim(1, power.trans): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n$par\n[1] 0.1673828\n\n$value\n[1] 6882400177\n\n$counts\nfunction gradient \n      28       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nShow code\nlambda = optim(1,power.trans)$par\n\n\nWarning in optim(1, power.trans): one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\nShow code\nlambda\n\n\n[1] 0.1673828\n\n\nShow code\n# checking transformation\n\nY.trans = (Y^lambda - 1)/(lambda)\nY.hat = lm(YX)$fit  \n\nmodel3 = lm(Y.trans~x1+x2+x3+x4+x5+x6+x7)\n\n#put transformed model into step function\n  #to remove any variables that arent significant\nmodel_new = step(model3, direction = \"backward\")\n\n\nStart:  AIC=-13.52\nY.trans ~ x1 + x2 + x3 + x4 + x5 + x6 + x7\n\n       Df Sum of Sq    RSS     AIC\n- x7    1     0.157 205.43 -15.339\n- x1    1     0.426 205.70 -15.034\n- x5    1     1.683 206.96 -13.615\n&lt;none&gt;              205.28 -13.518\n- x3    1     5.401 210.68  -9.467\n- x2    1    28.886 234.16  15.159\n- x6    1    81.071 286.35  62.037\n- x4    1   132.034 337.31 100.201\n\nStep:  AIC=-15.34\nY.trans ~ x1 + x2 + x3 + x4 + x5 + x6\n\n       Df Sum of Sq    RSS     AIC\n- x1    1     0.415 205.85 -16.870\n&lt;none&gt;              205.43 -15.339\n- x5    1     2.033 207.47 -15.045\n- x3    1     5.307 210.74 -11.396\n- x2    1    30.423 235.86  14.838\n- x6    1   118.758 324.19  88.958\n- x4    1   132.043 337.48  98.316\n\nStep:  AIC=-16.87\nY.trans ~ x2 + x3 + x4 + x5 + x6\n\n       Df Sum of Sq    RSS     AIC\n&lt;none&gt;              205.85 -16.870\n- x5    1     4.496 210.34 -13.835\n- x3    1     5.729 211.58 -12.474\n- x2    1    32.126 237.97  14.921\n- x4    1   133.007 338.85  97.266\n- x6    1   135.794 341.64  99.174\n\n\nShow code\n#part b stuff\nnew_car &lt;- data.frame(\n  x2 = 3.6,\n  x3 = 6,\n  x4 = 225,\n  x5 = 24,\n  x6 = 3237\n)\n\n#bootstrap for normality\n\ncar.m3 &lt;- data.frame(\n  x1 = x1,\n  x2 = x2,\n  x3 = x3,\n  x4 = x4,\n  x5 = x5,\n  x6 = x6,\n  x7 = x7\n)\n\n#Step 2, BS sample x and residuals (BSx, BSres)\nset.seed(123)  # for reproducibility\nn &lt;- nrow(car.m3)\nB &lt;- 10000  # number of bootstrap iterations\n\n# Store bootstrap predictions\nBS.ystar &lt;- numeric(B)\norig.res &lt;- residuals(model_new)\n\nfor(i in 1:B){ \n  # Step 1: Resample x and residuals \n  BS.index=sample(1:n,n,replace =TRUE) \n  BS.x=car.m3[BS.index,] \n  BS.res &lt;- sample(orig.res, n, replace = TRUE) \n  # Step 2: Predicted gy for bootstrap x \n  BS.yhat &lt;- predict(model_new, newdata = data.frame(x = BS.x)) \n  # Step 3: Generate BS transformed y\n  BS.y &lt;- BS.yhat + BS.res\n  \n  BS.data = Cars\n  BS.data[,3] = BS.y\n  BS.data[,c(2,4,5,6,7,8,9)] = BS.x\n  Y.trans = BS.y\n  BS_new_car &lt;- data.frame(\n    x2 = x2,\n    x3 = x3,\n    x4 = x4,\n    x5 = x5,\n    x6 = x6,\n    Y.trans = Y.trans)\n  \n  # Step 4: Fit bootstrap model\n  BS.model3 = lm(Y.trans~x2+x3+x4+x5+x6,data=BS_new_car)\n  \n  # Step 5: Predict transformed y for 80th percentile\n  BS.ystar[i] &lt;- predict(BS.model3, newdata = new_car) + sample(orig.res, 1)\n  \n}\n\npred_trans &lt;- quantile(BS.ystar, 0.80)\n  pred_price &lt;- ((pred_trans * lambda) + 1)^(1 / lambda)\nHey Frank and Tron,\nSo, full disclosure…I don’t know much about cars. But since I’ve got numbers on the brain and you’ve got customers, I thought I’d help out with pricing your cars without making you guess."
  },
  {
    "objectID": "Projects/IntDataAnalysis/CarPrices/Cars4Sale.html#part-a-predicting-the-normal-price",
    "href": "Projects/IntDataAnalysis/CarPrices/Cars4Sale.html#part-a-predicting-the-normal-price",
    "title": "Helping Frank & Tron Price Cars (Without Crashing the Dealership)",
    "section": "Part A: Predicting the “Normal” Price",
    "text": "Part A: Predicting the “Normal” Price\nYour family member (my professor), gave me a dataset and here’s what I did (bit of nerdy stuff):\n\nFitted a linear model\n\nChecked summary stats to see which specs actually matter.\nLooked at plots of residuals to make sure we weren’t completely off base.\nUsed AIC and ANOVA to keep the model neither too stupid nor too complicated.\n\nTransformed the response\n\nPrices are a little skewed, so I used a power transformation (it’s like a magic math wand that makes the numbers better)\nThis makes our predicted prices more realistic and less wild.\nI used a stepwise regression to figure out which of the variables really matter for predicting car prices. Turns out, almost everything was useful, except for x1 (Hybrid) and x7 (WheelBase), which didn’t add much predictive power, so the model ignored them.\n\nMade a function\n\nI wrote a function, predict_price(). You just feed it a car’s specs, and it spits out the average suggested retail price.\n\n\n\n\nShow code\n#PART A PREDICTION\npredict_price &lt;- function(hybrid = 0,\n                          engineSize = 2.9,\n                          cylinders = 6,\n                          horsepower = 200,\n                          highwayMPG = 29,\n                          weight = 3300,\n                          wheelBase = 107) {\n  # Create a new data frame for prediction\n  new_car &lt;- data.frame(\n    x1 = hybrid,\n    x2 = engineSize,\n    x3 = cylinders,\n    x4 = horsepower,\n    x5 = highwayMPG,\n    x6 = weight,\n    x7 = wheelBase\n  )\n  \n  # Predict on the transformed scale (same as model3’s Y)\n  predicted_trans &lt;- predict(model_new, newdata = new_car)\n  \n  # Back-transform using your power transformation lambda\n  if (lambda != 0) {\n    predicted_price &lt;- ((predicted_trans * lambda) + 1)^(1 / lambda)\n  } else {\n    predicted_price &lt;- exp(predicted_trans)  # special case if λ ≈ 0\n  }\n  \n  # Print a clean output\n  cat(\"Estimated Suggested Retail Price: $\", round(predicted_price, 2), \"\\n\")\n}\n\npredict_price(0, 3.0, 6, 382, 29, 3400, 97.2)\n\n\nEstimated Suggested Retail Price: $ 61548.13 \n\n\nBoom! A 2026 Toyota GR Supra 3.0 (base) priced without even leaving your chair.\n\nOkay… so the model’s off by a couple grand, but hey, for just looking at engine size, horsepower, weight, and MPG, it’s not too bad!"
  },
  {
    "objectID": "Projects/IntDataAnalysis/CarPrices/Cars4Sale.html#part-b-pricing-for-fancy-folks-80th-percentile",
    "href": "Projects/IntDataAnalysis/CarPrices/Cars4Sale.html#part-b-pricing-for-fancy-folks-80th-percentile",
    "title": "Helping Frank & Tron Price Cars (Without Crashing the Dealership)",
    "section": "Part B: Pricing for Fancy Folks (80th Percentile)",
    "text": "Part B: Pricing for Fancy Folks (80th Percentile)\nNow, since our Newport, Anaheim Hills, and Tustin customers are fancy, we don’t want to sell at the average prices. We want top of the line, but not insane. That’s where bootstrapping comes in:\n\nBootstrapping lets us simulate what the 80th percentile price would be by resampling the data and the residuals many times.\n\nTake a Cadillac we might show: 3.6L engine, 6 cylinders, 225 horsepower, 24 highway MPG, weighing 3,237 lbs, a 107-inch wheelbase, and not a hybrid. Bootstrapping it, our model suggests pricing it at about $27,490.\n\n\nShow code\n# Histogram of bootstrap predictions\nhist(BS.ystar,\n     breaks = 50,\n     main = \"Bootstrap Distribution of Predicted Cadillac Prices\",\n     xlab = \"Transformed Price\",\n     col = \"lightblue\")\n\n# Add a vertical line for the 80th percentile\nabline(v = pred_trans, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nThis histogram shows all the different prices our bootstrap model imagined for the Cadillac. The red line is the 80th percentile. Basically the price we’d want to aim for if we want to look fancy.\nFingers crossed this helps, Tron and Frank! Maybe after Thanksgiving break I’ll whip up a sick little calculator website for you guys but for now, midterms are taking over my life. :)"
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html",
    "href": "Projects/CompStats/Baseball/Baseball.html",
    "title": "Project 3: Bayesian Inference",
    "section": "",
    "text": "In baseball, “launch speed” refers to the speed (in mph) at which the baseball travels after the hitter hits it. Baseball superstar Aaron Judge tends to hit the ball hard. Like, 100 mph launch speed hard.\nOn June 3, 2023, Judge injured his right big toe crashing into the fence at Dodger Stadium and did not play again for almost two months. The objective of this project is to determine whether the injury and his subsequent recovery affected his launch speed and, if so, quantify how much of a difference there was between pre-injury and post-injury Judge."
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#the-objective",
    "href": "Projects/CompStats/Baseball/Baseball.html#the-objective",
    "title": "Project 3: Bayesian Inference",
    "section": "",
    "text": "In baseball, “launch speed” refers to the speed (in mph) at which the baseball travels after the hitter hits it. Baseball superstar Aaron Judge tends to hit the ball hard. Like, 100 mph launch speed hard.\nOn June 3, 2023, Judge injured his right big toe crashing into the fence at Dodger Stadium and did not play again for almost two months. The objective of this project is to determine whether the injury and his subsequent recovery affected his launch speed and, if so, quantify how much of a difference there was between pre-injury and post-injury Judge."
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#the-data",
    "href": "Projects/CompStats/Baseball/Baseball.html#the-data",
    "title": "Project 3: Bayesian Inference",
    "section": "The Data",
    "text": "The Data\n\n\nShow code\n# use this chunk to load any packages you need to do the analysis\n# e.g., ggplot2, janitor, purrr\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\n\nShow code\n# this code should work if you put this file in your Projects subfolder of Math 337\n# you may need to open it inside your Math 337 R project\nhere::i_am(\"Baseball.qmd\")\n\n# this code should work if you make a folder called Data in your Math 337 folder and put the data in it\njudge &lt;- readr::read_csv(here::here(\"Instruction_Data/judge.csv\"))\n\n\nThe data for this project comes from Baseball Savant and was obtained primarily using the baseballr package (Dr. Wynne did a bit of data wrangling with the originally obtained data). It contains launch speed data about the 55 baseballs Aaron Judge hit into play in the month before his injury and the 62 baseballs he hit into play in the month after returning from injury."
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#section-1-the-prior-distribution",
    "href": "Projects/CompStats/Baseball/Baseball.html#section-1-the-prior-distribution",
    "title": "Project 3: Bayesian Inference",
    "section": "Section 1: The Prior Distribution",
    "text": "Section 1: The Prior Distribution\nGoal 1:\nI aimed to explore what Aaron Judge’s launch speeds might look like before observing any data by simulating from a prior distribution. I adjusted the parameters so that the simulated speeds remained realistic. For example, rarely exceeding 120 mph, which a baseball expert (Gigi) confirmed is essentially impossible. I used histograms to visualize the results, allowing me to see plausible ranges and verify that the prior expectations were reasonable, thus providing a sensible starting point for the Bayesian analysis.\n\n\nShow code\n#general idea of what we want to get\n#y1: need mu1 and sigma\n  #mu1: t1, sigma, kappa0\n    #sigma: alpha0, lambda0\n#WLOG y2\n#set t1, t1, alpha0, lambda0, kappa0\n\n#set seed\nset.seed(1222)\nsim_prior_pred &lt;- function(R = 1000,\n                           t1 = 99,#expected mean preinjury\n                          t2 = 90,#expected mean postinjury\n                          kappa0 = 5,# strength of prior\n                      alpha0 = 30,#gamma shape 4 precision\n                      lambda0 = 1000#gamma rate 4 precision\n) {\n  #Step 1: Draw precision v from Gamma(alpha0/2, lambda0/2) to then get sigma\n  v2 &lt;- rgamma(R, shape = alpha0/2, rate = lambda0/2)\n  sigma &lt;- 1 / sqrt(v2)\n  \n  #Step 2: Draw mu1, mu2 given sigma and kappa0\n  mu1 &lt;- rnorm(R, mean = t1, sd = sigma / sqrt(kappa0))\n  mu2 &lt;- rnorm(R, mean = t2, sd = sigma / sqrt(kappa0))\n  \n  #step 3: Draw y1,y2 based on mu1/2 and sigma\n  y1 &lt;- rnorm(R, mean = mu1, sd = sigma) #pre-injury\n  y2 &lt;- rnorm(R, mean = mu2, sd = sigma) #post-injury\n  \n  #put it into a list to then make visuals for\n  list(sigma = sigma, mu1 = mu1, mu2 = mu2, y1 = y1, y2 = y2)\n}\n\n#run it\n#putting it into histograms\nres &lt;- sim_prior_pred()\nhist(res$y1) #pre\n\n\n\n\n\n\n\n\n\nShow code\nhist(res$y2) #post"
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#section-2-the-observed-data",
    "href": "Projects/CompStats/Baseball/Baseball.html#section-2-the-observed-data",
    "title": "Project 3: Bayesian Inference",
    "section": "Section 2: The Observed Data",
    "text": "Section 2: The Observed Data\nGoal 1:\nBased on the numerical and graphical summaries, Aaron Judge’s launch speeds appear slightly lower after the injury, but the change is not dramatic. The average launch speed decreased from 99.3 mph (pre-injury) to 97.9 mph (post-injury). The spread of the data also changed: pre-injury launch speeds had a slightly larger standard deviation (12.3) compared to post-injury (11.0), suggesting that his pre-injury performance was a bit less consistent. The boxplots reflect this, showing a slightly tighter cluster of launch speeds in the post-injury period.\nOverall, Judge’s post-injury launch speeds are slightly lower but also more consistent. So based on launch speed alone, he appears to be performing at a similar level pre- and post-injury.\n\n\nShow code\n#summarize data from the csv\nlaunch_summary &lt;- judge |&gt;\n  select(injury, launch_speed) |&gt; #selecting just these\n  group_by(injury) |&gt;\n  summarise(\n    mean = mean(launch_speed), #look at averages\n    median = median(launch_speed), #look at medians\n    sd = sd(launch_speed), #get the sd\n    min = min(launch_speed), #whats the lowest\n    max = max(launch_speed) #whats the highest\n  )\nlaunch_summary\n\n\n# A tibble: 2 × 6\n  injury  mean median    sd   min   max\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Post    98.0   100.  11.0  66.3  116.\n2 Pre     99.3   102   12.3  69.7  117.\n\n\nShow code\n#started with histograms but liekd the idea of a boxplot instead\n#setting the order for visual purposes\n  #cause og it was post on the left pre on the right\n    #not the biggest of deals but ya know... i cared\njudge$injury &lt;- factor(judge$injury, levels = c(\"Pre\", \"Post\")) #had to look up to get help on how to do this\n\nggplot(judge, aes(x = injury, y = launch_speed, fill = injury)) +\n  geom_boxplot() +\n  labs(\n    title = \"Launch Speed Pre vs Post Injury\",\n    y = \"Launch Speed (mph)\"\n  ) +\n  scale_fill_manual(values = c(\"pink\", \"lightblue\")) +\n  theme_light() \n\n\n\n\n\n\n\n\n\nShow code\n#ummmm why does he look more consistent afterwards?"
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#section-3-creating-the-posterior-distribution",
    "href": "Projects/CompStats/Baseball/Baseball.html#section-3-creating-the-posterior-distribution",
    "title": "Project 3: Bayesian Inference",
    "section": "Section 3: Creating the Posterior Distribution",
    "text": "Section 3: Creating the Posterior Distribution\nGoal 1:\nI used Gibbs sampling algorithm to generate posterior distributions for the pre-injury mean (mu1) and post-injury mean (mu 2) and standard deviation (sigma) of Judge’s launch speeds. The overall goal of the Gibbs sampler is to update one parameter at a time using the current values of the others, over and over, so that eventually the sequence of draws settles down and represents the full joint posterior distribution of all the parameters. In other words, by repeatedly sampling in this way, we let the algorithm “learn” the relationships between the parameters and produce realistic combinations of mu1, mu2, and sigma based on both our data and prior beliefs.\n\n\nShow code\n# set seef\nset.seed(1222)\n#parameters from prior predictive function from section 1\nt1 &lt;- 99\nt2 &lt;- 90\nkappa0 &lt;- 5\nalpha0 &lt;- 30\nlambda0 &lt;- 1000\n\n#splitting data to use:\ny1 &lt;- judge$launch_speed[judge$injury == \"Pre\"]\ny2 &lt;- judge$launch_speed[judge$injury == \"Post\"]\nn1 &lt;- length(y1)\nn2 &lt;- length(y2)\n\n#storage matrix\nparam &lt;- matrix(0, nrow = 5000, ncol = 3)\n\n#initial values from data\nmu1 &lt;- mean(y1)\nmu2 &lt;- mean(y2)\nsigma &lt;- sd(c(y1,y2))\n\n#Gibbs Sampelr\n#iterating 5000 times\nfor (i in 1:5000){\n  #sample mu1 given mu 2 and sigma\n  mu_p1 &lt;- (sum(y1) + kappa0*t1) / (n1 + kappa0)\n  sd_p1 &lt;- sigma / sqrt(n1 + kappa0)\n  mu1 &lt;- rnorm(1, mean = mu_p1, sd = sd_p1) #updated mu1\n  \n  #same for mu2 given mu 1 sigma\n  mu_p2 &lt;- (sum(y2) + kappa0*t2) / (n2 + kappa0)\n  sd_p2 &lt;- sigma / sqrt(n2 + kappa0)\n  mu2 &lt;- rnorm(1, mean = mu_p2, sd = sd_p2) #updated mu2\n  \n  #Sample sigma given mu1 and mu2\n  #putting it in parts cauuuuse i caaan\n  #precision v = 1/sigma^2\n  \n  #post shape based off of formula sheet\n  alpha_p &lt;- alpha0 + n1 + n2\n  \n  #doing all this shabam for lambda_p based on formula sheet\n  # Compute Sums + prior terms (the kappa segment)\n  sum1 &lt;- sum((y1 - mu1)^2)\n  sum2 &lt;- sum((y2 - mu2)^2)\n  kappa_segment &lt;- kappa0*((mu1 - t1)^2 + (mu2 - t2)^2)\n  \n  lambda_p &lt;- lambda0 + sum1 + sum2 + kappa_segment\n  \n  #sample v\n  v &lt;- rgamma(1, shape = alpha_p/2, rate = lambda_p/2)\n  sigma &lt;- 1/sqrt(v) #precision thing i commented earlier\n  \n  #store results\n  param[i,] &lt;- c(mu1, mu2, sigma)\n}\n\n#burn in removal\nparam_converged &lt;- param[1001:5000,]\n\n#plots for goal 2\n#par(mfrow=c(2,2))\nplot(param_converged[,1], type=\"l\", main=\"mu1\")\n\n\n\n\n\n\n\n\n\nShow code\nplot(param_converged[,2], type=\"l\", main=\"mu2\")\n\n\n\n\n\n\n\n\n\nShow code\nplot(param_converged[,3], type=\"l\", main=\"sigma\")\n\n\n\n\n\n\n\n\n\nGoal 2:\nAfter discarding the first 1,000 burn-in iterations, I plotted the posterior samples for mu1, mu2, and sigma. Each plot shows how the sampled values evolve over the iterations. The plots fluctuate around stable levels without any noticeable trends or drifts, which indicates that the chains have likely converged to their stationary distributions."
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#section-4-interpreting-the-posterior-distribution",
    "href": "Projects/CompStats/Baseball/Baseball.html#section-4-interpreting-the-posterior-distribution",
    "title": "Project 3: Bayesian Inference",
    "section": "Section 4: Interpreting the Posterior Distribution",
    "text": "Section 4: Interpreting the Posterior Distribution\nGoal 1:\nThe posterior distribution of the difference in launch speeds (mu1 - mu2) is mostly positive, suggesting that pre-injury launch speeds were generally higher than post-injury. A small fraction of draws are negative but overall the updated beliefs indicate a slight decrease in launch speed following the injury.\n\n\nShow code\n# get last 4000 draws\nmu1_post &lt;- param_converged[,1] #mu1 is pre\nmu2_post &lt;- param_converged[,2] #mu 2 is post\n\n# Compute posterior difference\ndiff_post &lt;- mu1_post - mu2_post\n\n#plot\nhist(diff_post,\n     breaks = 40,\n     col = \"lightblue\",\n     main = \"Posterior Distribution of mu1 − mu2\",\n     xlab = \"mu1 − mu2\")\n\n\n\n\n\n\n\n\n\nShow code\n#90% credible interval\n#taking quantile from diff_post which is the difference of mu1 and mu 2\nquantile(diff_post, c(0.05,0.95))\n\n\n       5%       95% \n-1.288514  5.147772 \n\n\nGoal 2:\nThe 90% credible interval for mu1 − mu2 ranges from about -1.3 to 5.1 mph. This suggests that Judge’s pre-injury launch speed was generally slightly higher than post-injury. However, there is some uncertainty, meaning we cannot be completely sure of the exact difference. While a small decrease is most likely, it’s still possible that the launch speed stayed the same. Overall, it is most likely that the injury caused a minor decrease in launch speed, but the effect is small and not dramatic.\nGoal 3:\nBased on the posterior distribution and the 90% credible interval for mu1 − mu2, Judge’s launch speeds post-injury are very similar to those pre-injury. While there is some indication of a small decrease in launch speed, the effect is minor. Overall, the analysis suggests that Judge returned as essentially the same player in terms of launch speed, with any changes being relatively minor rather than life-changing."
  },
  {
    "objectID": "Projects/CompStats/Baseball/Baseball.html#appendix-conceptual-details",
    "href": "Projects/CompStats/Baseball/Baseball.html#appendix-conceptual-details",
    "title": "Project 3: Bayesian Inference",
    "section": "Appendix: Conceptual Details",
    "text": "Appendix: Conceptual Details\nBayesian inference is a way of updating your beliefs about something uncertain using both what you already know and the new data you see. Before looking at the data, we start with a prior distribution, which is basically our guess or expectation. For example, I thought Aaron Judge’s average launch speed might be around 99 mph based on my very limited baseball knowledge. The likelihood function tells us how likely the data we actually saw is, given different possible values. Basically, how well different guesses match the real launch speeds. The posterior distribution combines the prior and the data to give an updated guess, showing what values are most likely after seeing the evidence. This way, we can mix what we thought before with what we actually observed to make a more informed conclusion."
  },
  {
    "objectID": "Projects/DataScience/FlowerProject.html",
    "href": "Projects/DataScience/FlowerProject.html",
    "title": "Modeling Encelia Species Project",
    "section": "",
    "text": "Show code\nlibrary(here)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\nlibrary(yardstick)\nlibrary(tidyr)\nlibrary(gt)\nlibrary(broom)"
  },
  {
    "objectID": "Projects/DataScience/FlowerProject.html#what-is-logistic-regression",
    "href": "Projects/DataScience/FlowerProject.html#what-is-logistic-regression",
    "title": "Modeling Encelia Species Project",
    "section": "What is logistic regression?",
    "text": "What is logistic regression?\nLogistic regression is a statistical modeling technique used to predict the probability of an event occurring. In our case, it predicts the probability that a flower belongs to a particular species. Logistic regression uses the logistic function to ensure predictions fall between 0 and 1, representing the likelihood of the flower being classified as one species or the other."
  },
  {
    "objectID": "Projects/DataScience/FlowerProject.html#why-are-you-doing-logistic-regression",
    "href": "Projects/DataScience/FlowerProject.html#why-are-you-doing-logistic-regression",
    "title": "Modeling Encelia Species Project",
    "section": "Why are you doing logistic regression?",
    "text": "Why are you doing logistic regression?\nI’m using logistic regression to classify the species of the Encelia flower based on their measured traits, like number of rays, disk diameter, ray diameter, and stem length. Since there are just two species (Californica vs. Farinosa), logistic regression helps me model how these traits relate to the chance that a flower belongs to one species or the other. This way, I can figure out which traits matter most for telling them apart and predict the species of new flowers based on their measurements.\n\n\nShow code\nencelia_prediction &lt;- function(split){\n  train &lt;- analysis(split)\n  valid &lt;- assessment(split)\n  \n  glm_all &lt;- glm(Species ~ number_rays + disk_diameter + ray_diameter + stem_length, data = train, family = \"binomial\")\n  glm_stemray &lt;- glm(Species ~ stem_length + ray_diameter, data = train, family = \"binomial\")\n  glm_null &lt;- glm(Species ~ 1, data = train, family = \"binomial\")\n  valid_predictions &lt;- valid |&gt;\n    mutate(\n      pred_all = predict(glm_all, newdata = valid, type = \"response\"),\n      pred_stemray = predict(glm_stemray, newdata = valid, type = \"response\"),\n      pred_null = predict(glm_null, newdata = valid, type = \"response\")\n    )\n  return(valid_predictions)\n}"
  },
  {
    "objectID": "Projects/DataScience/FlowerProject.html#why-did-you-choose-each-model-that-you-are-considering",
    "href": "Projects/DataScience/FlowerProject.html#why-did-you-choose-each-model-that-you-are-considering",
    "title": "Modeling Encelia Species Project",
    "section": "Why did you choose each model that you are considering?",
    "text": "Why did you choose each model that you are considering?\nI picked three models to see how well different traits can predict the species.\nAll traits model (pred_all): This one uses all the traits we measured (rays, disk diameter, ray diameter, and stem length) to see how well the full set works for telling the species apart.\nRays and stem length model (pred_stemray): I chose just rays and stem length because those showed the biggest differences between the species in the data. This simpler model checks if these two key traits alone can still make good predictions.\nNull model (pred_null): This one doesn’t use any traits at all. It just guesses based on how common each species is. It’s a baseline to compare the other models against.\nBy comparing these, I can figure out which traits really matter and if the simpler model does almost as well as the full one."
  },
  {
    "objectID": "Projects/DataScience/FlowerProject.html#why-are-you-using-cross-validation-how-does-it-work",
    "href": "Projects/DataScience/FlowerProject.html#why-are-you-using-cross-validation-how-does-it-work",
    "title": "Modeling Encelia Species Project",
    "section": "Why are you using cross-validation? How does it work?",
    "text": "Why are you using cross-validation? How does it work?\nI’m using cross-validation to see how well my models will work on new data they haven’t seen before. Instead of just testing on the same data the model was trained on, cross-validation splits the data into chunks (called folds) and tests the model on each chunk while training it on the rest.\n\n\nShow code\nset.seed(1)\nencelia_cv &lt;- vfold_cv(\n  encelia_train, \n  v = 4\n  )\n\n\n\n\nShow code\nmapped_predictions &lt;- map(\n  encelia_cv$splits,\n  encelia_prediction\n)\nmapped_predictions_df &lt;- mapped_predictions|&gt;\n  bind_rows(\n    .id = \"fold\"\n  )\nmapped_predictions_df|&gt;\n  select(Species, fold, pred_all, pred_stemray, pred_null)\n\n\n# A tibble: 79 × 5\n   Species     fold   pred_all pred_stemray pred_null\n   &lt;fct&gt;       &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 Californica 1      2.22e-16      0.0317      0.458\n 2 Californica 1      2.22e-16      0.0720      0.458\n 3 Californica 1      2.22e-16      0.0185      0.458\n 4 Californica 1      2.22e-16      0.00106     0.458\n 5 Californica 1     NA            NA           0.458\n 6 Californica 1      2.22e-16      0.0354      0.458\n 7 Farinosa    1      1   e+ 0      0.986       0.458\n 8 Farinosa    1      2.22e-16      0.286       0.458\n 9 Farinosa    1      2.22e-16      0.920       0.458\n10 Farinosa    1      1   e+ 0      0.955       0.458\n# ℹ 69 more rows\n\n\nShow code\npred_all &lt;- mapped_predictions_df |&gt;\n  pivot_longer(\n    cols = starts_with(\"pred\"),\n    names_to = \"model\",\n    values_to = \".pred_Farinosa\"\n  ) |&gt;\n  mutate(\n    .pred_Californica = 1 - .pred_Farinosa\n  )\n\npred_all |&gt;\n  select(Species, model, fold, .pred_Californica, .pred_Farinosa)\n\n\n# A tibble: 237 × 5\n   Species     model        fold  .pred_Californica .pred_Farinosa\n   &lt;fct&gt;       &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n 1 Californica pred_all     1                 1           2.22e-16\n 2 Californica pred_stemray 1                 0.968       3.17e- 2\n 3 Californica pred_null    1                 0.542       4.58e- 1\n 4 Californica pred_all     1                 1           2.22e-16\n 5 Californica pred_stemray 1                 0.928       7.20e- 2\n 6 Californica pred_null    1                 0.542       4.58e- 1\n 7 Californica pred_all     1                 1           2.22e-16\n 8 Californica pred_stemray 1                 0.981       1.85e- 2\n 9 Californica pred_null    1                 0.542       4.58e- 1\n10 Californica pred_all     1                 1           2.22e-16\n# ℹ 227 more rows\n\n\n\n\nShow code\nbrier_all_models &lt;- pred_all |&gt;\n  group_by(model, fold) |&gt;\n  brier_class(\n    truth = Species,\n    .pred_Californica\n  )\nbrier_summary &lt;- brier_all_models |&gt;\n  ungroup() |&gt;\n  group_by(model) |&gt;\n  summarize(\n    mean_brier = mean(.estimate),     \n    se_brier = sd(.estimate) / sqrt(n())  \n  ) |&gt;\n  arrange(mean_brier)\nbrier_summary|&gt;\n  gt() |&gt;\n  tab_header(title = \"Model Comparison\") |&gt;\n  cols_align(align = \"left\") |&gt;\n  opt_stylize(style = 6, color = \"green\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(20),\n    heading.subtitle.font.size = px(14),\n    data_row.padding = px(3),\n    table.width = pct(100)\n  )\n\n\n\n\n\n\n\n\nModel Comparison\n\n\nmodel\nmean_brier\nse_brier\n\n\n\n\npred_stemray\n0.1023426\n0.014271252\n\n\npred_all\n0.1233803\n0.055310803\n\n\npred_null\n0.2598904\n0.004411057\n\n\n\n\n\n\n\nBased on the Brier score, I’m choosing the pred_stemray model as the best one. It had the lowest average Brier score (0.1023), meaning its predicted probabilities were the closest to the actual species. It also had a low standard error (0.0143), which shows the model performed consistently across the different data splits. This model only uses ray count and stem length (the two traits with the biggest differences between species) so it’s both accurate and easy to understand."
  },
  {
    "objectID": "Projects/EAGER/Blog.html",
    "href": "Projects/EAGER/Blog.html",
    "title": "Heat Shock, Cancer Style",
    "section": "",
    "text": "In a study from CSU Fullerton, scientists used transcriptomic data to explore how human cells respond to heat shock at the molecular level. Their findings revealed both expected and unexpected stress responses, offering new insight into how cells cope under pressure (Reinschmidt et al., 2023).\n\n\nWhat is Heat Shock Response?\n\n\n\n\n\n\nRecall the time you were sick?\n\n\n\nYou might have a had fever. This caused your body to undergo alot of stress, but instead of shutting down, it activates protective systems to help you recover and keep going.\n\n\nAll cells experience some form of stress, whether from heat, toxins, or infections. Heat stress, in particular, can cause proteins inside cells to misfold. When this happens, cells activate the Heat Shock Response (HSR), their built-in survival system.\n\n\n\nWhat are Heat Shock Proteins?\nOnce the HSR is triggered, cells begin producing special proteins called heat shock proteins (HSPs). Their job includes:\n\nHelp proteins fold correctly\nRepair damaged ones\nPrevent future protein misfolding\n\n\n\n\n\n\n\nHow is this Relevant?\n\n\n\nIn the CSU Fullerton study, researchers observed the expected activation of classic heat shock proteins like HSP70 and HSP90. But they also found activation of non-canonical responses which are genes not traditionally associated with heat shock (Reinschmidt et al., 2023).\n\n\nThis suggests the HSR is more complex and far-reaching than we previously thought, with variations depending on the cell type.\n\n\n\nCancer Cells Survival Strategy\n\n\n\n\n\n\nThe Problem with Cancer Cells\n\n\n\nCancer cells are under constant stress. Low oxygen, exposure to radiation, and chemotherapy are just a few. But instead of dying under this pressure (as treatments intend), cancer cells ramp up their production of HSPs. Cancer cells hijack the heat shock response to avoid death.\n\n\nBy overactivating the HSR, cancer cells become practically immune, withstanding chemotherapy, radiation, and other treatments that would normally kill them. This ability to exploit the heat shock response is part of what makes cancer so difficult to treat.\n\n\n\nOur Offense to Cancers Defense\nThe discovery of non-canonical HSR pathways opens new possibilities for cancer treatment. If we can figure out how to selectively block the amplified heat shock response in cancer cells, we could make them more vulnerable to therapy.\nIn other words, disabling cancer’s stress-coping tricks could improve the effects of chemotherapy and other treatment outcomes. Understanding how cancer cells produce HSPs differently than normal cells is key to reaching the full potential of cancer therapy.\n\n\n\n\n\n\nTakeaway\n\n\n\nThis research shows how basic cell biology is important to real-world medicine. By uncovering new information on heat shock response, scientists have identified new targets to research that could be used to outsmart cancer and possibly other diseases that exploit stress responses\n\n\n\n\n\nInfographic Recap\n\n\n\n\n\n\n\nReferences\n\nReinschmidt, A., Solano, L., Chavez, Y., Hulsy, W. D., & Nikolaidis, N. (2023). Transcriptomics unveil canonical and non-canonical heat shock-induced pathways in human cell lines. International Journal of Molecular Sciences, 26(3), 1057. https://doi.org/10.3390/ijms26031057"
  },
  {
    "objectID": "Classes/DataScience.html",
    "href": "Classes/DataScience.html",
    "title": "Foundations of Data Science (Math 237)",
    "section": "",
    "text": "This course introduced the foundations of data science through hands-on work with real datasets. We learned how to collect, clean, and visualize data; build introductory statistical models; make predictions; and communicate results using reproducible coding practices."
  },
  {
    "objectID": "Classes/DataScience.html#projects",
    "href": "Classes/DataScience.html#projects",
    "title": "Foundations of Data Science (Math 237)",
    "section": "Projects",
    "text": "Projects\n\nSoCal Rent Project\nIn this project, I analyzed rental prices across several Southern California counties to understand how factors like square footage, number of bedrooms/bathrooms, and regional differences influence price.\n➡View Project\n\n\nEncelia Flower Project\nFor this project, I collected data from the Fullerton Arboretum on two native California species—Encelia californica and Encelia farinosa. I performed exploratory analysis and built simple linear regression models to compare characteristics between the two species.\n➡ View Project"
  },
  {
    "objectID": "Classes/UC_Berkeley.html",
    "href": "Classes/UC_Berkeley.html",
    "title": "Modeling the Role of Selfing Plants in Assisted Gene Flow",
    "section": "",
    "text": "This ongoing research project, conducted during my Summer 2025 appointment at UC Berkeley, explores how self-fertilizing plant populations respond to assisted gene flow (AGF) interventions under climate-driven selection pressures. I work with the MOI Lab at UC Berkeley, and this page documents the project’s motivation, modeling framework, early visuals, and research communication materials. Here, you can find my poster, conceptual diagrams, and links to presentations. The modeling workflow is ongoing, and outputs will be updated as simulations and analyses are completed."
  },
  {
    "objectID": "Classes/UC_Berkeley.html#background-motivation",
    "href": "Classes/UC_Berkeley.html#background-motivation",
    "title": "Modeling the Role of Selfing Plants in Assisted Gene Flow",
    "section": "Background & Motivation",
    "text": "Background & Motivation\nAssisted gene flow (AGF) is a conservation strategy that introduces beneficial genetic variation into populations threatened by climate change. While AGF has been widely explored in outcrossing species, the dynamics in self-fertilizing (selfing) plants remain less understood.\nBecause climate change is rapidly altering environmental conditions, understanding whether AGF can realistically provide adaptive benefits to selfing populations is an urgent question. Modeling provides a cost-effective and flexible way to explore these evolutionary outcomes before field testing."
  },
  {
    "objectID": "Classes/UC_Berkeley.html#research-questions",
    "href": "Classes/UC_Berkeley.html#research-questions",
    "title": "Modeling the Role of Selfing Plants in Assisted Gene Flow",
    "section": "Research Questions",
    "text": "Research Questions\nThis project focuses on the following key questions:\n\nHow does selfing rate influence the spread of introduced adaptive alleles?\nUnder what demographic and environmental conditions does AGF improve long-term population persistence?\nHow do selection strength, migration input, and selfing rates interact?\nWhat is the potential for outbreeding depression to limit the success of AGF?"
  },
  {
    "objectID": "Classes/UC_Berkeley.html#methods-modeling-approach",
    "href": "Classes/UC_Berkeley.html#methods-modeling-approach",
    "title": "Modeling the Role of Selfing Plants in Assisted Gene Flow",
    "section": "Methods & Modeling Approach",
    "text": "Methods & Modeling Approach\nI used SLiM, a population genetics simulation program, to model how self-fertilizing plants respond to assisted gene flow. The simulations look at:\n\nHow climate affects the original population\nIntroducing new, beneficial alleles into the population\nDifferent rates of self-fertilization versus outcrossing\nPossible negative effects from mixing populations (outbreeding depression)\n\n\nPlanned Components\n\nMeasure fitness\nAdjust selfing rates from fully outcrossing to fully selfing\nControl how many individuals are added via assisted gene flow\nSet population size, either constant or density-dependent\nRun the simulation for 100–300 generations\n\nSLiM outputs data that I then analyze in R. This includes things like how allele frequencies change over time and summaries of the population, which help me understand how selfing and assisted gene flow affect these plants."
  },
  {
    "objectID": "Classes/UC_Berkeley.html#poster-lightning-talk",
    "href": "Classes/UC_Berkeley.html#poster-lightning-talk",
    "title": "Modeling the Role of Selfing Plants in Assisted Gene Flow",
    "section": "Poster & Lightning Talk",
    "text": "Poster & Lightning Talk\nA PDF of my research poster will be inserted here. Download Poster\nAdd short slides linked here: Lightning Talk – Google Slides"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! I’m Natalie Torres, a math major at Cal State Fullerton, graduating in Spring 2026. I’m especially drawn to data science as I love working with messy data, finding patterns, and turning analyses into clean, meaningful visualizations.\nI’m currently part of Project EAGER, where I’ve been exploring genomics, computational analysis, and research communication. This year, I presented my first research poster at SCCUR, which made me excited about pursuing graduate school in statistics or data science.\nOutside of academics, I’m all about gaming (Stardew Valley, Minecraft, and recently Balatro) and collecting way too many stickers.\nI’m building this portfolio to share what I’ve learned, what I’m working on, and where I’m hoping to grow next."
  },
  {
    "objectID": "index.html#find-me-online",
    "href": "index.html#find-me-online",
    "title": "About Me",
    "section": "Find Me Online",
    "text": "Find Me Online\n\nGitHub: @Nat-Tor\n\nLinkedIn: Natalie Torres\n\nEmail: natalietor@csu.fullerton.edu\n\n\nThis portfolio is a work in progress so things are a little messy but more projects, updates, and results will be added over time. &lt;3"
  },
  {
    "objectID": "Classes/EAGER.html",
    "href": "Classes/EAGER.html",
    "title": "EAGER",
    "section": "",
    "text": "The NIH-funded Project EAGER at CSU Fullerton provides undergraduate students with hands-on experience in genomics, bioinformatics, and research communication. Through workshops, seminars, and mentored research, we explored how computational tools can be used to study biological systems and big data in modern biology.\nLearn more about the program here\nAs part of this program, I conducted summer research at UC Berkeley, where I worked with genomic data and developed bioinformatics workflows.\n➡ UC Berkeley Research Page"
  },
  {
    "objectID": "Classes/EAGER.html#projects",
    "href": "Classes/EAGER.html#projects",
    "title": "EAGER",
    "section": "Projects",
    "text": "Projects\n\nHeat Stress & Cancer Cells — Research Blog\nThis blog post summarizes my exploration of how heat stress influences protein expression pathways in cancer cells. I reviewed findings from transcriptomic studies and discussed the biological mechanisms linking heat shock responses to cancer progression.\n➡ View Blog\n\n\nRNA-seq Pipeline (Work in Progress)\nI am currently developing an R-based pipeline to reproduce the analysis used to examine protein and gene expression under heat stress conditions. This pipeline includes data preprocessing, quality control, normalization, and exploratory analysis steps, and will be updated as I complete more of the workflow. I hope for it to be useful to give future EAGER cohorts a base of genomic tools in R ➡ (Page coming soon!)"
  },
  {
    "objectID": "Classes/IntDataAnalysis.html",
    "href": "Classes/IntDataAnalysis.html",
    "title": "Intermediate Data Analysis (Math 439)",
    "section": "",
    "text": "Math 439 focuses on building a deeper understanding of statistical modeling, with an emphasis on regression, model diagnostics, ANOVA, and modern computational techniques. The course covers the mathematical foundations of least squares, simple and multiple regression, variable selection, model checking, bootstrapping, and introductory Bayesian treatments of regression models. Throughout the semester, we applied these methods to real datasets using R and developed practical skills in evaluating and improving statistical models."
  },
  {
    "objectID": "Classes/IntDataAnalysis.html#projects",
    "href": "Classes/IntDataAnalysis.html#projects",
    "title": "Intermediate Data Analysis (Math 439)",
    "section": "Projects",
    "text": "Projects\n\nCar Prices Project\nIn this project, I used regression modeling to predict used car prices based on dealership data. I compared multiple predictors, assessed model fit, and evaluated which factors most strongly influenced hypothetical sale prices.\n➡View Project\n\n\nCOVID Lung Capacity Project\nThis analysis investigated whether individuals who were vaccinated had higher post-COVID lung capacity compared to unvaccinated individuals. Using bootstrapping and model diagnostics, I examined differences between groups and evaluated the strength of the evidence.\n➡View Project\n\n\nEarthquake Project\nThis project analyzed mainshock–aftershock pairs from the Atacama Fault Zone using regression techniques. I applied transformations and model diagnostics to address issues such as nonlinearity while studying how the magnitude of a mainshock influences its aftershock.\n➡View Project"
  },
  {
    "objectID": "Classes/CompStats.html",
    "href": "Classes/CompStats.html",
    "title": "Computational Statistics (Math 337)",
    "section": "",
    "text": "Math 337 serves as a bridge between introductory data science and advanced applied statistics courses. The class focuses on computational and conceptual tools used in modern statistics, including simulation, bootstrap methods, Bayesian inference, confidence intervals, and hypothesis testing. We also explored optimization techniques and reviewed the mathematical properties of random variables, skills that support more advanced modeling work in later courses."
  },
  {
    "objectID": "Classes/CompStats.html#projects",
    "href": "Classes/CompStats.html#projects",
    "title": "Computational Statistics (Math 337)",
    "section": "Projects",
    "text": "Projects\n\nFortnite Microtransactions Project (Confidence Intervals)\nIn this project, I used bootstrap simulation and computational methods to construct and interpret confidence intervals related to Fortnite player microtransaction behavior.\n➡View Project\n\n\nBaseball Project (Bayesian Inference)\nThis project applied Bayesian methods to baseball performance data, exploring how prior beliefs combine with observed data to produce posterior distributions and predictions.\n➡ View Project ### Long COVID Project (Hypothesis Testing)\nIn this analysis, I conducted hypothesis testing using permutation and bootstrap-based methods to investigate questions related to Long COVID and differences across groups.\n➡View Project"
  },
  {
    "objectID": "Projects/DataScience/SoCalRentProject.html",
    "href": "Projects/DataScience/SoCalRentProject.html",
    "title": "Rent in Southern California",
    "section": "",
    "text": "Objective\nMy goal is to take a broad look at the rental market across these Southern California counties. I want to explore what each county offers in terms of rent and identify any trends that emerge from the listings. Specifically, I’m interested in how different priorities (square footage, number of bedrooms and bathrooms, or overall price) affect where someone might choose to rent.\nI understand that preferences vary, so rather than claiming one county is “best,” my goal is to highlight the trends I observe in each area and offer insights into what renters might expect based on their needs.\n\n\nPackages Used and Data Import\n\n\nShow code\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gt)\nlibrary(scales)\nlibrary(broom)\nhere::i_am(\"SoCalRentProject.qmd\")\nSoCalRent &lt;- read_csv(here::here(\"ProjectData/SoCalDataRaw_2025.csv\"))\n\n\n\n\nData Collection\nFinding clean and usable public data turned out to be more challenging than expected. After exploring several sites, I’ve decided to use Redfin. I then relied on Excel’s web tools to auto-generate tables directly from listing URLs. For each listing, I’ve included the Price($), Numbers of Bathrooms and Bedrooms, Square Footage, Address, Housing Type, and County. I focused on three Southern California counties: Orange County(OC), Riverside County(RIV), and Los Angeles County(LA). Across the three counties, I gathered data on three different property types: houses, condos, and apartments. While the raw data I pulled was relatively structured, it still required some cleaning.\n\n\nShow code\nSoCalRent |&gt;\n  head() |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Sample of Rent Listings Data\"\n  ) |&gt;\n  cols_label(\n    Price = \"Rent ($)\",\n    Beds = \"Bedrooms\",\n    Baths = \"Bathrooms\",\n    SqFt = \"Square Footage\",\n    Address = \"Address\",\n    Type = \"Housing Type\",\n    County = \"County\"\n  ) |&gt;\nopt_stylize(style = 6, color = \"cyan\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(18),\n    table.width = pct(100))\n\n\n\n\n\n\n\n\nSample of Rent Listings Data\n\n\nRent ($)\nBedrooms\nBathrooms\nSquare Footage\nAddress\nHousing Type\nCounty\n\n\n\n\n$3,195/mo\n4 beds\n2.5 baths\n1668.00\n4001 Landau Ct, Riverside, CA 92501\nhouse\nRIV\n\n\n$4,000/mo\n4 beds\n2 baths\n2200.00\n18590 Roberts Rd, Riverside, CA 92508\nhouse\nRIV\n\n\n$2,300/mo\n2 beds\n1 bath\n900.00\n3468 Wallace St, Jurupa Valley, CA 92509\nhouse\nRIV\n\n\n$3,449/mo\n3 beds\n2.5 baths\n1,970\n531 Atwood Ct, Riverside, CA 92506\nhouse\nRIV\n\n\n$1,850/mo\n1 bed\n1 bath\n400\n2767 Attenborough Dr, Riverside, CA 92503\nhouse\nRIV\n\n\n$1,950/mo\n1 bed\n1 bath\n440\n10671 Sagittarius Dr, Riverside, CA 92503\nhouse\nRIV\n\n\n\n\n\n\n\n\n\nData Wrangling\nThere were a few listings with missing data in certain columns, so for the sake of sanity, I just removed them.\nPrice came in various formats (e.g., $3,185/mo, $1940+/mo), so I removed dollar signs, commas, and plus signs, then converted the column to numeric.\nBeds and Baths also had inconsistent formats like 1 bed, 3 beds, or even ranges like 2–4 beds. I stripped out the words “bed(s)” and “bath(s)”, and for ranges, I took the smallest number. Both columns were then converted to numeric.\nSqFt was the messiest. It appeared as 1668.00, 1,970, or ranges like 410–480. I removed commas and decimal points (since I didn’t need the .00), and again, for ranges, I used the lower bound. This column was also converted to numeric.\nFor the address column, I kept it in case I wanted to refer to the full address later. But I also extracted just the city name into a separate column in case I wanted to analyze trends by city.\nLastly, I removed any listings where either Beds or Baths were equal to zero. This may affect apartment listing as it would have removed any studios that don’t have bedrooms.\nAfter cleaning, I was left with 278 observations out of the original 333.\n\n\nShow code\nRent_Clean &lt;- SoCalRent |&gt;\n  #cleaning out any listings that have any empty data for the sake of my sanity\n  filter(!is.na(Price), !is.na(Beds), !is.na(Baths), !is.na(SqFt), !is.na(Address), !str_starts(SqFt, \"—\")) |&gt;\n  mutate(\n  # Price column: remove any $, +, /mo and change it to a numeric variable\n  Price = str_remove_all(Price, \"\\\\$|,|/mo|\\\\+\"),\n  Price = as.numeric(Price),\n  # Beds column: take the smallest number of beds and change it to a numeric variable\n  Beds = str_extract(Beds, \"^\\\\d+\"),\n  Beds = as.numeric(Beds),\n  # Baths column: take the smallest number of baths and change it to a numeric variable\n  Baths = str_extract(Baths, \"^\\\\d+(\\\\.\\\\d+)?\"),\n  Baths = as.numeric(Baths),\n  # SqFt column: takeing the first number from ranges, removes and commas or dashes and change it to a numeric variable\n  SqFt = str_remove_all(SqFt, \",\"),  # Remove commas\n  SqFt = case_when(str_detect(SqFt, \"-\") ~ as.numeric(str_extract(SqFt, \"^[0-9]+\")), TRUE ~ as.numeric(SqFt)),\n  # City Column: I decided to just make a new column \"City\"\n  City = str_split_fixed(Address, \",\", 3)[, 2]\n  ) |&gt;\n  # If beds or baths is 0, remove the row\n  filter(Beds != 0, Baths != 0)\n\n\n\n\nShow code\nRent_Clean |&gt;\n  head() |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Sample of Clean Rent Listings Data\"\n  ) |&gt;\n  cols_label(\n    Price = \"Rent ($)\",\n    Beds = \"Bedrooms\",\n    Baths = \"Bathrooms\",\n    SqFt = \"Square Feet\",\n    Address = \"Address\",\n    Type = \"Housing Type\",\n    County = \"County\",\n    City = \"City\"\n  ) |&gt;\nopt_stylize(style = 6, color = \"cyan\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(18),\n    table.width = pct(100))\n\n\n\n\n\n\n\n\nSample of Clean Rent Listings Data\n\n\nRent ($)\nBedrooms\nBathrooms\nSquare Feet\nAddress\nHousing Type\nCounty\nCity\n\n\n\n\n3195\n4\n2.5\n1668\n4001 Landau Ct, Riverside, CA 92501\nhouse\nRIV\nRiverside\n\n\n4000\n4\n2.0\n2200\n18590 Roberts Rd, Riverside, CA 92508\nhouse\nRIV\nRiverside\n\n\n2300\n2\n1.0\n900\n3468 Wallace St, Jurupa Valley, CA 92509\nhouse\nRIV\nJurupa Valley\n\n\n3449\n3\n2.5\n1970\n531 Atwood Ct, Riverside, CA 92506\nhouse\nRIV\nRiverside\n\n\n1850\n1\n1.0\n400\n2767 Attenborough Dr, Riverside, CA 92503\nhouse\nRIV\nRiverside\n\n\n1950\n1\n1.0\n440\n10671 Sagittarius Dr, Riverside, CA 92503\nhouse\nRIV\nRiverside\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\nI analyzed the data by grouping listings by County and Housing Type.\nOverall, most listings were houses. Riverside County had the highest number of apartment listings, while Orange County had the most condo listings. When it comes to house listings, Riverside and Los Angeles counties were tied for the highest count.\n\n\nShow code\nRent_Clean |&gt;\n  count(Type, County) |&gt;\n  ggplot(aes(x = Type, y = n, fill = County)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  geom_text(aes(label = n), \n            position = position_dodge(width = 0.8), \n            vjust = -0.3, size = 3) +\n  scale_fill_manual(\n  values = c(\"LA\" = \"#005596\", \"OC\" = \"#FF6720\", \"RIV\" = \"#FFB81C\")\n)+\n  labs(\n    title = \"Most Common Housing Types by County\",\n    x = \"Housing Type\", \n    y = \"Number of Listings\"\n  ) +\n  theme_light() +\n  theme(\n    legend.position = \"top\",\n    text = element_text(size = 16),             \n    axis.title = element_text(size = 18),        \n    axis.text = element_text(size = 14),         \n    plot.title = element_text(size = 18, face = \"bold\"),\n    legend.title = element_text(size = 16),     \n    legend.text = element_text(size = 14)   \n  )\n\n\n\n\n\n\n\n\n\nNext, I made a table showing the average and median rent prices by county and housing type. As expected, apartments were the most affordable option across all counties, with Riverside having the lowest average price. On the other end, houses in Orange County stood out as the most expensive, with an average rent of $7,388. Condo prices varied widely, suggesting a mix of both luxury and more affordable units across different areas.\nIt was important to notice the difference between average and median values. This indicates that there exists high-priced outliers that are skewing the averages. Median values offer a better sense of typical rent prices.\n\n\nShow code\nsummary_table &lt;- Rent_Clean |&gt;\n  group_by(County, Type)|&gt;\n  summarize(mean.price = round(mean(Price, na.rm = TRUE)), \n            median.price = median(Price, na.rm = TRUE),\n            .groups = \"drop\")\nsummary_table |&gt;\n  arrange(desc(mean.price))|&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Average Rent Price by County and Type\") |&gt;\n  fmt_currency(\n    columns = c(mean.price, median.price),\n    currency = \"USD\",\n    decimals = 0) |&gt;\n  cols_label(\n    County = \"County\",\n    Type = \"Housing Type\",\n    mean.price = \"Average Rent\",\n    median.price = \"Median Price\") |&gt;\n  data_color(\n    columns = c(mean.price),\n    colors = col_numeric(\n      palette = c(palette = c(\"#2ecc71\", \"#f1c40f\", \"#e74c3c\")),\n      domain = NULL\n    )\n  ) |&gt;\n  cols_align(align = c(\"left\"))|&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(20),\n    heading.subtitle.font.size = px(14),\n    data_row.padding = px(3),\n    table.width = pct(100))\n\n\n\n\n\n\n\n\nAverage Rent Price by County and Type\n\n\nCounty\nHousing Type\nAverage Rent\nMedian Price\n\n\n\n\nOC\nhouse\n$7,388\n$6,850\n\n\nLA\nhouse\n$6,547\n$5,995\n\n\nOC\ncondo\n$4,385\n$4,000\n\n\nLA\ncondo\n$3,673\n$3,500\n\n\nLA\napartment\n$3,525\n$2,950\n\n\nRIV\nhouse\n$3,169\n$3,400\n\n\nOC\napartment\n$2,595\n$2,535\n\n\nRIV\ncondo\n$2,479\n$2,400\n\n\nRIV\napartment\n$1,885\n$1,865\n\n\n\n\n\n\n\nTo better visualize the distribution of rent prices and identify outliers, I created a boxplot grouped by County and Housing Type. The red dots represent outlier listings, which are values that fall significantly above the average rent range.\nThis visualization shows that Orange County and Los Angeles have several high-priced outliers, particularly for houses and condos. These extreme values help explain why the average rent is much higher than the median in those areas.\n\n\nShow code\nRent_Clean |&gt; \n  ggplot(aes(x = Type, y = Price, fill = County)) +\n  geom_boxplot(outlier.shape = 16, outlier.colour = \"#e74c3c\", outlier.size = 3) +\n  labs(\n    title = \"Rent Price Distribution by County and Housing Type\",\n    x = \"County\",\n    y = \"Rent Price ($)\"\n  ) +\n  scale_fill_manual(\n    values = c(\"LA\" = \"#005596\", \"OC\" = \"#FF6720\", \"RIV\" = \"#FFB81C\")\n  ) +\n  scale_y_continuous(trans = \"log10\") +  # Log scale to reduce skew\n  theme_light() +\n  theme(\n    legend.position = \"top\",\n    text = element_text(size = 16),             \n    axis.title = element_text(size = 18),        \n    axis.text = element_text(size = 14),         \n    plot.title = element_text(size = 16, face = \"bold\"),\n    legend.title = element_text(size = 16),     \n    legend.text = element_text(size = 14)   \n  )\n\n\n\n\n\n\n\n\n\nThese patterns in the number of listings and rent prices reveal the differences in the housing market across Southern California. Riverside’s high number of apartment listings and lower average rents point to its role as a more affordable and accessible housing market. Riverside has few outliers, suggesting more consistency in pricing and older/cheaper properties.\nIn contrast, Los Angeles and Orange County show multiple high-end outliers, especially for houses and condos, which increase the average prices. These outliers hint at the presence of luxury markets or high-demand neighborhoods.\n\n\nHow to define what gets filtered\nFor this project, I want to approach rental affordability from a personal angle, while keeping it general enough that others could adapt it to their own situations. A common rule of thumb is that individuals shouldn’t spend more than 30% of their income on rent. Based on this, I’ll introduce a salary variable to simulate what someone can realistically afford. This variable will help filter out listings that exceed that 30% income threshold.\nTo make the analysis more flexible, especially for listings with multiple bedrooms, I’m assuming that shared housing is an option. That means I’ll consider a unit affordable if the rent per bedroom is within 30% of a person’s monthly income. This way, I can include multi-bedroom housing options that might be affordable when split among roommates.\n\n\nShow code\nsalary &lt;- 60000  # random yearly salary\nmonthly_income &lt;- salary / 12 #calculating monthly paycheck assuming monthly rent payments\nmax_rent &lt;- (monthly_income * 0.30) #only 30% of income to be used for rent\nRent_Clean_Affordable &lt;- Rent_Clean |&gt; #filter out to only include the affordable listings\n  mutate(\n    rent_per_room = Price / Beds, #allowing for roomates\n    affordable = rent_per_room &lt;= max_rent #makes logical\n  )|&gt;\n  filter(affordable== TRUE) #filter\nsummary_affordable &lt;- Rent_Clean_Affordable |&gt; #making a new mean/median table but consider affordability\n  group_by(County, Type)|&gt;\n  summarize(mean.price = round(mean(Price, na.rm = TRUE)), \n            median.price = median(Price, na.rm = TRUE),\n            .groups = \"drop\")\nsummary_affordable |&gt; #make a new visually appealing table\n  arrange(desc(mean.price))|&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Average Rent Price by County and Type\") |&gt;\n  fmt_currency(\n    columns = c(mean.price, median.price),\n    currency = \"USD\",\n    decimals = 0) |&gt;\n  cols_label(\n    County = \"County\",\n    Type = \"Housing Type\",\n    mean.price = \"Average Rent\",\n    median.price = \"Median Price\") |&gt;\n  data_color(\n    columns = c(mean.price),\n    colors = col_numeric(\n      palette = c(palette = c(\"#2ecc71\", \"#f1c40f\", \"#e74c3c\")),\n      domain = NULL\n    )\n  ) |&gt;\n  cols_align(align = c(\"left\"))|&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(20),\n    heading.subtitle.font.size = px(14),\n    data_row.padding = px(3),\n    table.width = pct(100))\n\n\n\n\n\n\n\n\nAverage Rent Price by County and Type\n\n\nCounty\nHousing Type\nAverage Rent\nMedian Price\n\n\n\n\nLA\nhouse\n$4,360\n$4,400\n\n\nOC\nhouse\n$4,211\n$4,098\n\n\nOC\ncondo\n$3,630\n$3,958\n\n\nRIV\nhouse\n$3,369\n$3,450\n\n\nLA\ncondo\n$3,276\n$3,500\n\n\nLA\napartment\n$2,970\n$2,798\n\n\nOC\napartment\n$2,775\n$2,775\n\n\nRIV\ncondo\n$2,568\n$2,542\n\n\nRIV\napartment\n$1,926\n$1,950\n\n\n\n\n\n\n\nAfter adding the salary variable, I was able to realistically filter out the extreme outliers. After removing these high rent listings the average rent moved closer to the median. This suggests that the original averages were heavily influenced by a small number of expensive listings.\nThis adjustment is especially important for model building. By filtering out outliers, we create a dataset that better represents the usual trends, making our model more reliable.\n\n\nMultiple Linear Regression Model\nI made a multiple linear regression model to analyze how rent prices in Southern California are influenced by factors including county, housing type, number of bedrooms, bathrooms, and square footage. The model reveals several important relationships.\n\n\nShow code\nrent_model &lt;- lm(Price ~ County + Type + SqFt + Beds + Baths, \n                 data = Rent_Clean_Affordable) #the model :D\nrent_model_table &lt;- rent_model |&gt; tidy() #the model into table\n\nrent_model_table &lt;- rent_model_table |&gt; #mutate p.val to highlight significance\n  mutate(p.value = case_when(\n    p.value &lt;= 0.05 ~ \"≤ 0.05\",    # Highly significant\n    p.value &gt; 0.05 ~ \"&gt; 0.05\",     # Non-significant\n    TRUE ~ as.character(p.value)\n  ),\n  Change_Price = case_when( #highlight changes in price\n    estimate &gt; 0 ~ \"increase\",  # Positive coefficient (increase)\n    estimate &lt; 0 ~ \"decrease\",  # Negative coefficient (decrease)\n    TRUE ~ \"neutral\"            # Neutral\n  ))\n\nrent_model_table |&gt; #make pretty table\n  gt() |&gt;\n  tab_header(\n    title = \"Regression Results for Rent Model\"\n  ) |&gt;\n  cols_label(\n    term = \"Variable\",\n    estimate = \"Estimate\",\n    std.error = \"Standard Error\",\n    statistic = \"t-Statistic\",\n    p.value = \"p-Value\",\n    Change_Price = \"Change in Price\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Coefficients\",\n    columns = c(estimate, std.error, statistic, p.value)\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#2ecc71\"),\n    locations = cells_body(columns = vars(estimate), \n                           rows = Change_Price == \"increase\")\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#e74c3c\"),\n    locations = cells_body(columns = vars(estimate), \n                           rows = Change_Price == \"decrease\")\n  ) |&gt;\n    tab_style(\n    style = cell_text(color = \"#e74c3c\"),\n    locations = cells_body(columns = vars(p.value), \n                           rows = p.value == \"&gt; 0.05\") #heavy sig\n  ) |&gt;\n  tab_style(\n    style = cell_text(color = \"#2ecc71\"),\n    locations = cells_body(columns = vars(p.value),  #non sig\n                           rows = p.value == \"≤ 0.05\")\n  ) |&gt;\n   cols_align(align = c(\"left\"))|&gt;\nopt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(18),\n    table.width = pct(100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression Results for Rent Model\n\n\nVariable\n\nCoefficients\n\nChange in Price\n\n\nEstimate\nStandard Error\nt-Statistic\np-Value\n\n\n\n\n(Intercept)\n1028.1934539\n219.212166\n4.6904032\n≤ 0.05\nincrease\n\n\nCountyOC\n188.9374670\n143.341713\n1.3180913\n&gt; 0.05\nincrease\n\n\nCountyRIV\n-718.4149778\n111.538787\n-6.4409431\n≤ 0.05\ndecrease\n\n\nTypecondo\n73.2123135\n172.780024\n0.4237314\n&gt; 0.05\nincrease\n\n\nTypehouse\n401.2195019\n188.448517\n2.1290669\n≤ 0.05\nincrease\n\n\nSqFt\n-0.1666292\n0.155018\n-1.0749019\n&gt; 0.05\ndecrease\n\n\nBeds\n844.4871509\n97.993519\n8.6177857\n≤ 0.05\nincrease\n\n\nBaths\n70.5854399\n108.488020\n0.6506289\n&gt; 0.05\nincrease\n\n\n\n\n\n\n\nAn apartment in LA is our baseline. Listings in the OC area are higher than LA (by $189) but it is not statistically significant. Listing in the Riverside area tend to cost less than in Los Angeles, this difference is statistically significant.\nThe number of bedrooms had the strongest and most statistically significant association with rent. Each additional bedroom increases rent by approximately $844 on average. Baths are not as statistically significant only increasing rent by $70.\nHouses rent for approximately $401 more than apartments but shown by the model, doesn’t have. Condos are not statistically significant, only increasing rent by $73.\nAlthough Square Footage is often expected to impact rent, in this model it is not accurate. Suggesting other factors may play a bigger role (i.e. bedrooms). This model can indicate that the number of bedrooms may play a more important role in rent pricing than square footage, possibly because people value how many individuals a space can house more than the size itself.\n\n\nShow code\nPrediction &lt;- rent_model|&gt;\n  augment( newdata= Rent_Clean_Affordable)\n# Plot the residuals\nggplot(data = Prediction, aes(x = .resid)) +\n  geom_histogram(bins = 10, fill = \"#005596\", color = \"black\", alpha = 0.8) +\n  labs(title = \"Histogram of Residuals\",\n       x = \"Residuals\",\n       y = \"Frequency\") +\n  theme_light() +\n  theme(\n    legend.position = \"top\",\n    text = element_text(size = 16),             \n    axis.title = element_text(size = 18),        \n    axis.text = element_text(size = 14),         \n    plot.title = element_text(size = 16, face = \"bold\"),\n    legend.title = element_text(size = 16),     \n    legend.text = element_text(size = 14)   \n  )\n\n\n\n\n\n\n\n\n\nThis histogram visualizes the distribution of residuals from the multiple linear regression model. The residuals are mostly centered around zero, indicating that the model generally makes accurate predictions. However, there is a slight skew to the left, suggesting that the model tends to overestimate rent prices for a number of listings.\n\n\nConclusion\nAfter exploring the data using multiple methods, I was able to uncover key housing trends across each of the Southern California counties. As someone who has lived here for over 20 years, many of these patterns align with my lived experience but it’s pretty cool to see data analytics prove what me and other locals often see.\nLos Angeles (LA):\nAs expected, LA is a heavily populated area and has some of the highest rent prices. The correlation analysis shows that square footage has a stronger impact on rent in LA, likely due to the high demand and limited space (many people, not much room). The histogram also reveals a right-skewed distribution, with prices trailing into the more expensive range, reinforcing the idea of LA as a competitive and costly housing market.\nRiverside (RIV):\nRiverside’s histogram displays lower rent values. Its status as an older, more suburban region contributes to its relative affordability. The correlation table shows a balanced influence from the number of beds, baths, and square footage on pricing. This suggests that Riverside follows more predictable pricing patterns. It reflects a suburban area offering more consistent and accessible housing options.\nOrange County (OC):\nUnlike LA and Riverside, Orange County’s histogram doesn’t show a clear skew, indicating a wide spread of rent prices. This suggests a mix of older, more affordable homes and newer, high-end properties. The weaker correlation between basic housing features and rent price implies that other external factors (school quality, neighborhood amenities, community in general) may play a more important role in determining rent in Orange County.\n\n\nShow code\ncorrelation_table &lt;- Rent_Clean |&gt;\n  group_by(County) |&gt;\n  summarise(\n    Bed_cor = cor(Price, Beds), \n    Baths_cor = cor(Price, Baths), \n    SqFt_cor = cor(Price, SqFt)\n  )\ncorrelation_table |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Correlation of Rent Price with Features by County\"\n  ) |&gt;\n  cols_label(\n    County = \"County\",\n    Bed_cor = \"Price vs Beds\",\n    Baths_cor = \"Price vs Baths\",\n    SqFt_cor = \"Price vs SqFt\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(columns = everything())\n  ) |&gt;\n   cols_align(align = c(\"left\"))|&gt;\nopt_stylize(style = 6, color = \"cyan\") |&gt;\n  tab_options(\n    table.font.size = px(16),\n    heading.title.font.size = px(18),\n    table.width = pct(100))\n\n\n\n\n\n\n\n\nCorrelation of Rent Price with Features by County\n\n\nCounty\nPrice vs Beds\nPrice vs Baths\nPrice vs SqFt\n\n\n\n\nLA\n0.5704509\n0.4944130\n0.7294382\n\n\nOC\n0.4496779\n0.5389367\n0.5983870\n\n\nRIV\n0.9013643\n0.7824197\n0.7990179\n\n\n\n\n\n\n\n\n\nShow code\nRent_Clean |&gt;\n  filter(Price &lt; 10000)|&gt;\n  mutate(County = as.character(County)) |&gt;\n  bind_rows(\n    Rent_Clean_Affordable |&gt;\n      mutate(County = \"All Counties\")\n  ) |&gt;\n  ggplot(aes(x = Price, fill = County)) +\n  geom_histogram(binwidth = 800, color = \"black\") +\n  facet_wrap(~ County, scales = \"free_y\") +\n  labs(\n    title = \"Rent Price Distribution: Overall and by County\",\n    x = \"Rent Price ($)\",\n    y = \"Number of Listings\"\n  ) +\n  scale_fill_manual(\n    values = c(\"LA\" = \"#005596\", \"RIV\" = \"#FFB81C\",\"OC\" = \"#FF6720\", \n      \"All Counties\" = \"gray\" \n    )\n  ) +\n  theme_light() +\n  theme(\n    text = element_text(size = 16),             \n    axis.title = element_text(size = 18),        \n    axis.text = element_text(size = 14),         \n    plot.title = element_text(size = 16, face = \"bold\"),\n    legend.title = element_text(size = 16),     \n    legend.text = element_text(size = 14)   \n  )\n\n\n\n\n\n\n\n\n\nWhich brings me back to my objective: Where should someone rent? As mentioned earlier, the answer depends on individual priorities. If you’re looking for consistent affordability, Riverside is a solid choice with its more predictable and accessible pricing. The LA area, while expensive, reflects the appeal and allure of the area beyond just housing. As for Orange County, it’s great for those seeking a balance between affordability and amenities. It offers a mix of options for people who value a comfortable lifestyle with access to good schools, parks, and community features.\nUltimately, there’s no one-size-fits-all answer. Renters must weigh their priorities, whether it’s budget, space, amenities, or proximity to work and culture, in order to find the right fit for them.\n\n\nLimitations\nEven though this project helped uncover some cool insights about rent trends in Southern California, there were a few limitations worth mentioning.\nData Source:\nThe data I used was based on the rental listings available during this time (APR 2025) and like most online listings, they change fast. Some of the ones I pulled were already taken down a couple of weeks later. On top of that, there were some missing or weird values in the dataset. For example, some listings had 0 bedrooms, which probably meant they were studios. But the way the data was structured (and for simplicity as a beginner), I chose to remove them. I remember that decision actually cut out a decent number of apartment listings from LA, which probably skewed the overall picture there a bit.\nModeling Assumptions:\nI used a linear regression model, which assumes a straight-line relationship between things like square footage, beds, and baths, and the rent price. But let’s be real, housing markets are way more complicated than that. There are tons of other factors that influence rent like location, neighborhood vibe, schools, commute times and those things are hard to capture with the dataset I had. So while the model gives us a starting point, it definitely doesn’t tell the whole story.\n\n\nReflection\nThis project was genuinely a lot of fun. Working with real housing data pushed me to practice key skills like data cleaning and wrangling. I felt way more confident in organizing messy data and turning it into something meaningful. I especially enjoyed creating the visuals and being able to actually see the trends and share them in a clear way.\nI also connected with this project on a personal level. I’ve lived in Southern California my whole life, so it was really interesting to analyze an everyday topic like rent through the lens of a data analyst. It helped me understand how prices vary and what factors play a role in shaping the housing landscape across different counties.\nThere were definitely a few more things I would’ve liked to explore or experiment with, but I decided to focus on cleaning and improving the work I already had. Honestly, just being able to turn data into visuals and insights was exciting enough.\n\n\nEXTRA: my favorite weird listings\n217 E 29th St Unit 217 and 217 1/2, Los Angeles, CA 90011: Upon further investigation, this is actually a listing for two units with a total of 4 beds and 4 baths—so 2 beds and 2 baths per unit. The listing also mentions that each bedroom can house 2–3 people. Based on how my code calculates rent per person, that would come out to around $187/month, give or take a roommate or two.\nAs amazing as that rent price sounds, it definitely raises some eyebrows. On the bright side, it could be a legit affordable housing option for someone who really needs it. But on the more skeptical side… the listing gives off some sketchy vibes. Personally, I’m not interested.\nRental Description: Our brand new, all-inclusive shared rooms are move-in ready just bring your bag and settle into comfort, convenience, and community in the heart of the city. All-inclusive rates starting at just $700/month only 2-3 person per room ! Move in today stay as long or as little as you need All utilities Wi-Fi included Weekly cleaning of shared spaces Private ensuite bathrooms in most rooms On-site laundry for easy living Street & paid parking available On-site manager for support when you need it No credit checks. No deposits. No SSN. Just a valid ID from any country.Spots are limited and going fast message now and claim your bed today!”\n\n1313 Disneyland Dr., Anaheim, CA 92802: This listing was up for rent at $30,000/month and completely skewed my average rental price for Orange County. At first, I assumed it was just a fancy condo near Disneyland. But when I looked into it, I realized it was actually inside Disneyland. Weird.\nCurious (and knowing there are plenty of Disneyland superfans out there), I searched Reddit and sure enough, 1313 Disneyland Dr. is the park’s official address. Fun fact: M is the 13th letter of the alphabet, so 1313 = MM = Mickey Mouse. I couldn’t find much else about what this listing actually was.\nThe listing had been up for nearly a year and only got taken down recently on April 30th, 2025. My best guess? Maybe it was the apartment above the fire station that Walt Disney supposedly used. But more realistically, it could’ve been something related to private entertainment or internal use within Disneyland.\nStill, kind of strange that the listing disappeared just two weeks after I pulled the data.\nRental Description: This property is available. Please inquire on this site to schedule a showing. Im sorry, I cant provide a description of this property.Available NOWHeating ForcedAirCooling EvaporativeAppliances Refrigerator, Range Oven, Microwave, DishwasherLaundry In UnitParking Detached Garage, 2 spacesPets Dogs Allowed, Cats AllowedSecurity deposit 30,000.00Included Utilities Garbage, Sewage, LandscapingAdditional DepositPet 500.00This property has a home security system.Disclaimer Ziprent is acting as the agent for the owner. Information Deemed Reliable but not Guaranteed. All information should be independently verified by renter."
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html",
    "title": "Project 1: Confidence Intervals",
    "section": "",
    "text": "Microtransactions are small “add-ons” that video game players can purchase, including new cosmetic options for the player’s character, items, “loot boxes” that contain randomized content, etc. Unlike a traditional model, which sells the entire game for a fixed price, microtransaction-based business models place no theoretical upper limit on the amount of money a player can spend over the lifetime of the game.\nResearchers are interested in estimating the average amount spent by Fortnite players on microtransactions in a typical month. Because Fortnite is a game played all over the world, they ensure that their sample of roughly 100 Fortnite players is also global. The one researcher on the team that remembers any of their introductory statistics constructs a 95% one-sample t confidence interval of (\\(\\$24.48, \\$47.92\\)). However, when looking up how to interpret this interval, they find some resources that claim that they should not be doing this procedure because, although the sample size is at least 40, there are some extreme outliers in their data. They have asked you to help them come up with a better confidence interval and interpret what that interval means."
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#the-objective",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#the-objective",
    "title": "Project 1: Confidence Intervals",
    "section": "",
    "text": "Microtransactions are small “add-ons” that video game players can purchase, including new cosmetic options for the player’s character, items, “loot boxes” that contain randomized content, etc. Unlike a traditional model, which sells the entire game for a fixed price, microtransaction-based business models place no theoretical upper limit on the amount of money a player can spend over the lifetime of the game.\nResearchers are interested in estimating the average amount spent by Fortnite players on microtransactions in a typical month. Because Fortnite is a game played all over the world, they ensure that their sample of roughly 100 Fortnite players is also global. The one researcher on the team that remembers any of their introductory statistics constructs a 95% one-sample t confidence interval of (\\(\\$24.48, \\$47.92\\)). However, when looking up how to interpret this interval, they find some resources that claim that they should not be doing this procedure because, although the sample size is at least 40, there are some extreme outliers in their data. They have asked you to help them come up with a better confidence interval and interpret what that interval means."
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#the-data",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#the-data",
    "title": "Project 1: Confidence Intervals",
    "section": "The Data",
    "text": "The Data\n\n\nShow code\n# use this chunk to load any packages you need to do the analysis, e.g., ggplot2 or MASS\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\n\nShow code\n# this code should work if you make a folder called Projects in your Math 337 folder and put this file in it\n# you may need to open it inside your Math 337 R project\nhere::i_am(\"Fornite_MicroTransactions.qmd\")\n# this code should work if you make a folder called Data in your Math 337 folder and put the fortnite_spend data in it\nfortnite_spend &lt;- fortnite_spend &lt;- read_csv(\"Instructions_Data/fortnite_spend.csv\")\n\n\nThe data used comes from Etchells, Morgan, and Quintana (2022), “Loot box spending is associated with problem gambling but not mental wellbeing,” and was collected via survey advertised on Reddit, Twitter, and Science Focus magazine during Fall 2020 and Spring 2021. It is unknown to what extent this sample represents Fortnite players in 2025."
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#section-1-is-their-t-confidence-interval-okay",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#section-1-is-their-t-confidence-interval-okay",
    "title": "Project 1: Confidence Intervals",
    "section": "Section 1: Is Their t-Confidence Interval Okay?",
    "text": "Section 1: Is Their t-Confidence Interval Okay?\nGoal 1: Using a nonparametric bootstrap, I generated 100 resampled datasets from the original purchase data and then calculated the mean for each sample to estimate the sampling distribution of the sample mean.\nThis results in the distribution having a mean of 36.17 and a standard deviation of 6.27, which shows the variance we might see in the samples of the sample mean.\n\n\nShow code\n## Goal 1\n# set seed\nset.seed(314) \n\nB &lt;- 100                 # number of bootstrap samples\nn &lt;- nrow(fortnite_spend)  # sample size (number of obs in data (206 for this))\n\n# Generate bootstrap samples from purchase data\n# Each row of the matrix represents one bootstrap sample of size n=106\nmean_boot_sample &lt;- matrix(\n  sample(fortnite_spend$purchase, size = n * B, replace = TRUE),\n  nrow = B\n)\n\n# Getting the mean for each bootstrap sample\n# theta_star is a vector of all the means aka the sampling distribution???\ntheta_star &lt;- apply(mean_boot_sample, 1, mean)\n\n# Summarize the bootstrap sampling distribution\nmean(theta_star)    # Mean of the bootstrap \n\n\n[1] 36.17134\n\n\nShow code\nsd(theta_star)      # Standard deviation of bootstrap\n\n\n[1] 6.272001\n\n\nShow code\n# Plot: the histogram looks pretty normal (compared to what they had), \n# meaning we can now get better estimates\nhist(theta_star, main = \"Bootstrap Distribution of Sample Mean\",\n     xlab = \"Mean Purchase Amount\")\n\n# red line showing the og sample mean\nabline(v = mean(fortnite_spend$purchase), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nGoal 2: The histogram of purchase amounts is strongly right skewed with an obvious outlier. This shows that the data is not normally distributed. Even though the t-test produced a confidence interval, it would not be right to assume a normal sampling distribution for the sample mean. This is why I suggest to use the nonparametric method above to get a more reliable estimate.\n\n\nShow code\n## Goal 2\n\n# Running a one-sample t-test on the purchase data\n# assumes the sampling distribution of the mean is normal\nt.test(fortnite_spend$purchase)\n\n\n\n    One Sample t-test\n\ndata:  fortnite_spend$purchase\nt = 6.1264, df = 105, p-value = 1.59e-08\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 24.48388 47.91631\nsample estimates:\nmean of x \n 36.20009 \n\n\nShow code\n# Histogram of data shows strong right skew and outliers indicating that the normal \n# assumption may not hold\nggplot(fortnite_spend, aes(x = purchase)) +\n  geom_histogram(binwidth = 15) +\n  labs(\n    title = \"Distribution of Purchase Amounts\"\n  )"
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#section-2-a-better-estimate",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#section-2-a-better-estimate",
    "title": "Project 1: Confidence Intervals",
    "section": "Section 2: A “Better” Estimate",
    "text": "Section 2: A “Better” Estimate\nGoal 1: Using the sample mean of the bootstrap sampling distribute, I estimate the population mean purchase amount to be $36.17. I also computed a 95% confidence interval giving and interval of about $26.19 to $50.01 . This confidence interval helps explain any variability that might pop up if I repeatedly sampled the original population.\n\n\nShow code\n## Goal 1\n# set seet\nset.seed(314)\n\n# population mean using the bootstrap sampling distribution\nmean(theta_star)  \n\n\n[1] 36.17134\n\n\nShow code\n# make a 95% confidence interval\nc &lt;- 0.95\nalpha &lt;- 1 - c\n\n# Using percentiles as the CI bounds\nquantile(theta_star, probs = c(alpha/2, 1 - alpha/2))  \n\n\n    2.5%    97.5% \n26.18693 50.00525 \n\n\nGoal 2: These estimates suggest that the average amount spent on microtransactions is about $36.17. The 95% confidence interval ($26.19 to $50.01) shows that the true average spending for the entire population of players likely is within this range. This hopefully gives the researchers a better idea of typical spending behavior while recognizing variability."
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#section-3-is-this-estimate-actually-better",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#section-3-is-this-estimate-actually-better",
    "title": "Project 1: Confidence Intervals",
    "section": "Section 3: Is This Estimate Actually “Better”?",
    "text": "Section 3: Is This Estimate Actually “Better”?\nGoal 1: Assuming the purchase amounts follow an exponential distribution, the rate parameter can be estimated as 1/(sample mean). Using the average spending ($36.17), the estimated rate is approximately 0.028, which can be used to model the exponential distribution of purchase amounts.\n\n\nShow code\n## Goal 1\n# get sample mean from the original purchase data\nx.bar &lt;- mean(fortnite_spend$purchase)  \n# x.bar = 36.17\n\n# For an exponential distribution, lambda (rate)= 1 / mean\nrate &lt;- 1 / x.bar \n\n\nGoal 2: The parametric bootstrap shows that 91.6% of the confidence intervals contain the true mean. This is slightly below the 95% confidence interval constructed earlier. This means even my confidence interval slightly underestimated the true coverage.\n\n\nShow code\n## Goal 2\n# function to compute bootstrap percentile confidence interval for mean\nboot_mean_percentile_ci &lt;- function(x, B = 100, C = 0.95){ \n  n &lt;- length(x)\n  \n  # Generate B bootstrap samples from x (data)\n  boot_sample &lt;- matrix(\n    sample(x, n*B, replace = TRUE),\n    nrow = B)\n  \n  # mean of each bootstrap sample\n  theta_star &lt;- apply(boot_sample, 1, mean)\n  \n  # get CI\n  alpha &lt;- 1 - C\n  perc_ci &lt;- quantile(theta_star, c(alpha/2, 1 - alpha/2))\n  \n  return(perc_ci)\n}\n\n# Generate parametric bootstrap data using the estimated exponential distribution\n#    1000 simulated datasets of size 100\nset.seed(314)\nbig_data &lt;- matrix(\n  round(rexp(1000*100, rate),2), \n  nrow = 1000, \n  ncol = 100)\n\n# apply bootstrap CI function\nboot_cis &lt;- apply(big_data, 1, boot_mean_percentile_ci)\n\n# put results into a data frame\nbounds &lt;- data.frame(low = boot_cis[1,], high = boot_cis[2,])\n\n# is the original mean covered by each CI\nbounds_sorted &lt;- bounds |&gt;\n  mutate(\n    index = 1:1000,\n    cover = (x.bar &gt;= low & x.bar &lt;= high)\n  )\n\n# summarize coverage\nbounds_sorted |&gt;\n  summarize(\n    too_low = mean(high &lt; x.bar),      # CI below true mean\n    too_high = mean(low &gt; x.bar),      # CI above true mean\n    just_right = mean(cover)           # CI has true mean\n  )\n\n\n  too_low too_high just_right\n1    0.06    0.024      0.916"
  },
  {
    "objectID": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#appendix-technical-details",
    "href": "Projects/CompStats/ForniteMicroTransactions/Fornite_MicroTransactions.html#appendix-technical-details",
    "title": "Project 1: Confidence Intervals",
    "section": "Appendix: Technical Details",
    "text": "Appendix: Technical Details\nA bootstrap works by resampling from the data given over and over. It recalculates the statistic (in this case the mean) each time to see how it differs. The nonparametric bootstrap does this by resampling directly from the data (section 1) while the parametric bootstrap, assumes a model for the data (section 3 with an exponential distribution) and generate new samples from that model."
  },
  {
    "objectID": "Projects/CompStats/LongCOVID/LongCOVID.html",
    "href": "Projects/CompStats/LongCOVID/LongCOVID.html",
    "title": "Project 2: Hypothesis Tests",
    "section": "",
    "text": "You are working for a pharmaceutical company that is testing a new treatment for long COVID. You are tasked with analyzing the collected data to determine whether there is sufficient evidence of an effect of treatment on the risk of long COVID in the at-risk population. Before any data is collected, you should investigate the sensitivity of your test to differences in the assumptions about the collected data, e.g., number of patients enrolled, true population proportion of long COVID among treated or untreated adults, etc."
  },
  {
    "objectID": "Projects/CompStats/LongCOVID/LongCOVID.html#the-objective",
    "href": "Projects/CompStats/LongCOVID/LongCOVID.html#the-objective",
    "title": "Project 2: Hypothesis Tests",
    "section": "",
    "text": "You are working for a pharmaceutical company that is testing a new treatment for long COVID. You are tasked with analyzing the collected data to determine whether there is sufficient evidence of an effect of treatment on the risk of long COVID in the at-risk population. Before any data is collected, you should investigate the sensitivity of your test to differences in the assumptions about the collected data, e.g., number of patients enrolled, true population proportion of long COVID among treated or untreated adults, etc."
  },
  {
    "objectID": "Projects/CompStats/LongCOVID/LongCOVID.html#the-data",
    "href": "Projects/CompStats/LongCOVID/LongCOVID.html#the-data",
    "title": "Project 2: Hypothesis Tests",
    "section": "The Data",
    "text": "The Data\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(purrr)\n\n\n\n\nShow code\n# this code should work if you put this file in your Projects subfolder of Math 337\n# you may need to open it inside your Math 337 R project\nhere::i_am(\"LongCOVID.qmd\")\n\n## the data from Maier, Kowalski-Dobson, et al.\nlong_covid &lt;- data.frame(\n  symptoms = c(rep(1, 23), rep(0, 106), rep(1, 81), rep(0, 214)),\n  # symptoms = 1 if symptoms remain after 90 days, 0 if symptoms are gone within 90 days\n  vax = c(rep(\"treatment\", 129), rep(\"control\", 295))\n  # treatment = vaccinated, control = unvaccinated\n)\n# table(), xtabs() and tabyl() (from janitor) will all produce tables of this data,\n# coercing both symptoms and treatment to factors automatically\n\n\nThe data in long_covid comes from Maier, Kowalski-Dobson, et al. (2024), “Reduction in long-COVID symptoms and symptom severity in vaccinated compared to unvaccinated adults,” and was collected on employees and students at the University of Michigan who tested positive for COVID-19 between October 2020 and December 2022. The specific data here is restricted to pre-Omicron variant COVID-19 (roughly October 2020 - December 2021)."
  },
  {
    "objectID": "Projects/CompStats/LongCOVID/LongCOVID.html#section-1-permutation-test-for-effect-of-vaccination",
    "href": "Projects/CompStats/LongCOVID/LongCOVID.html#section-1-permutation-test-for-effect-of-vaccination",
    "title": "Project 2: Hypothesis Tests",
    "section": "Section 1: Permutation Test for Effect of Vaccination",
    "text": "Section 1: Permutation Test for Effect of Vaccination\n\nGoal 1:\n\n\nShow code\n#like in class from estimating alphas\n#observing diff in proportions different then in class when we observed difference in means.\nperm_test &lt;- function(df, R = 999, alternative = \"t\") {\n#df: data\n  #column 1: response (0 = no long covid and 1 = long covid) (symptoms)\n  #column 2: group (treatment or control) (vax)\n#R: number of perm resamples to generate\n  # alternative: type of test (l = left, r = right, t = two-sided)\n  \n#check the alt argument is valid\n  assertthat::assert_that(\n    alternative %in% c(\"l\", \"r\", \"t\"),\n    msg = 'Alternative must be \"l\", \"r\", or \"t\" '\n  )\n  \n#get the response variable (long COVID yes 1/no 0)\n  response &lt;- df[, 1, drop = TRUE]\n  \n#get and convert the group variable to a factor (to separate treatment/control)\n  group &lt;- as.factor(df[, 2, drop = TRUE])\n  \n#storing group names\n  control &lt;- levels(group)[1] #control\n  treatment &lt;- levels(group)[2] #treatment (cause alphabetical order)\n  \n\n#STEP 1: Compute obs diff in proportions\n  #mean of 0 and 1 values is sample prop\n  #have to get proportions since numbers are binary and not like values\n  p1_obs_control &lt;- mean(response[group == control])  # props of long COVID in group1 (control)\n  p2_obs_treatment &lt;- mean(response[group == treatment])  # props of long COVID in group2 (treatment)\n  \n  #observed t.stat: diff in proportions\n  diff_obs &lt;- p1_obs_control - p2_obs_treatment\n  \n\n#STEP 2: Simulate permutation null dist\n\n  diff_samp_prop &lt;- function(ind, y) {\n  # ind: indices assigned to treatment group\n  #y: vector containing the response vals\n    #mean(y[ind]): simulated treatment group proportion\n    #mean(y[-ind]): simulated control group proportion\n    \n    diff_prop &lt;- mean(y[ind]) -  mean(y[-ind]) #diff in proportions\n    \n    return(diff_prop)\n  }\n  \n  #under the null hypothesis: group assignment doesnt matter\n  #so rand move around who is in g1 vs g2\n  index_sim &lt;- replicate(\n    R,\n    {\n      # randomly assign which patients are in the treatment group\n      sample(seq_along(response), sum(group == treatment), replace = FALSE)\n    }\n  )\n  \n  # --- part 3: compute simulated differences for each permutation ---\n  diff_props_sim &lt;- apply(\n    index_sim,   # matrix of simulated treatment indices\n    2,           # apply function to each column\n    diff_samp_prop, # custom helper function\n    y = response\n  )\n\n#STEP 3: Combine sim and obs stats\n  \n  diff_all &lt;- c(diff_props_sim, diff_obs)\n\n#STEP 4: Compute permutation pvals\n #left: probability of seeing a value &lt;= obs\n  p_left &lt;- mean(diff_all &lt;= diff_obs)\n  \n  #right: probability of seeing a value &gt;= observed\n  p_right &lt;- mean(diff_all &gt;= diff_obs)\n  \n  #two: smallest tail × 2\n  p_two_sided &lt;- min(p_left, p_right) * 2\n  \n#return results\n  invisible(\n    list(\n      obs_test_stat = diff_obs,   # observed difference in proportions\n      all_test_stat = diff_all,   # vector of simulated + observed stats\n      p.val = dplyr::case_when(   # pick correct p-value type\n        alternative == \"l\" ~ p_left,\n        alternative == \"r\" ~ p_right,\n        alternative == \"t\" ~ p_two_sided\n      )\n    )\n  )\n}\n  \n#testing:\nset.seed(1222)\nresults &lt;- perm_test(long_covid, R = 999, alternative = \"t\")\n\n#obs diff and p-value\nobs_diff &lt;- results$obs_test_stat   #observed diff\ndiff_all &lt;- results$all_test_stat\np_val &lt;- results$p.val   #pval\n\n#plot: histogram\nhist(\n  diff_all,\n  main = \"Permutation Null Distribution\",\n  xlab = \"Difference in Proportions (p1 - p2)\"\n)\n\n#observed statistic line\nabline(v = obs_diff, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\nGoal 2:\nThe observed difference in proportions between the control and treatment groups was 0.096. This means that the treatment group had a 9.6% lower risk of developing long COVID compared to the control group.\nThe null hypothesis assume that the treatment has no effect on the likelihood of developing long COVID. Comparing the observed difference resulted in a p-value of 0.014. Since this p-value is less than the significance level (0.05), then there is sufficient evidence to reject the null hypothesis. This means that getting treated will reduce the risk of developing long COVID."
  },
  {
    "objectID": "Projects/CompStats/LongCOVID/LongCOVID.html#section-2-analyzing-the-companys-new-treatment",
    "href": "Projects/CompStats/LongCOVID/LongCOVID.html#section-2-analyzing-the-companys-new-treatment",
    "title": "Project 2: Hypothesis Tests",
    "section": "Section 2: Analyzing the Company’s New Treatment",
    "text": "Section 2: Analyzing the Company’s New Treatment\n\nGoal 1:\nA type I error would mean that the company comes to the conclusion that treatment for long COVID does reduce the risk of long COVID when in reality it does not. The consequence would be a waste of time, money and resources. The company approving and promoting an ineffective treatment would lead to people buying something that wouldn’t work. This not only can impact the credibility of the company but also can stall further medical research from looking further into for (a better/an actual) effective treatment.\nA type II error would mean the company comes to the conclusion that the treatment doesn’t work when in reality it does reduce the risk of long COVID. The consequence would be delayed treatment, preventing patients from benefiting.\nWhile both errors are important, depending on the perspective, one might be worse then the other. From the patient point of view, type I error could be more harmful due to the risk of giving false information. From a company point of view, a type II error is more costly, spending more money trying to find a treatment that already exists.\nType I error in this context might be more serious due to the fact that it could impact credibility and has ethical consequences.\n\n\nGoal 2:\n\n\nShow code\nset.seed(1222)\np_values &lt;- replicate(\n  1000, #sim 1000 times\n  {\n    #null: both groups have same long COVID rate 20%\n      #using binom since its 0's and 1's\n    group1_response &lt;- rbinom(100, size = 1, prob = 0.20) \n    group2_response &lt;- rbinom(100, size = 1, prob = 0.20)\n    \n    #combine into df for perm_test\n    fake_data &lt;- data.frame(\n      response = c(group1_response, group2_response),\n      group = c(rep(\"treatment\", 100), rep(\"control\", 100))\n    )\n    \n    #perm test and get pval\n    perm_test_sim&lt;- perm_test(fake_data, R = 999, alternative = \"t\") |&gt;\n      purrr::pluck(\"p.val\")\n  }\n)\n\n#mean(p_values&lt;=0.05) this estimated alpha at just one pt 0.05\n\n# checking multiple alpha levels vvv\nalpha_vals &lt;- seq(0.01, 0.10, by = 0.01)\n\n# estimated Type I Error at each alpha using sapply\ntype1_error &lt;- sapply(alpha_vals, function(a) mean(p_values &lt;= a))\n\n# simple plot\nplot(alpha_vals, type1_error,\n     xlab = expression(alpha),\n     ylab = \"Estimated Type I Error Rate\",\n     main = \"Calibration of Permutation Test\")\nabline(0, 1, col = \"magenta\")  # ideal line y = x\n\n\n\n\n\n\n\n\n\n\n\nGoal 2:\nI first had to simulate the null hypothesis using the binomial distribution. This is because the response variables is binary (1’s and 0’s).\nThen evaluated the calibration of the permutation test by estimating type I error rate at multiple significant levels. The result was that the observed type I error is consistently a little below the nominal alpha level. By plotting the ECDF of these p-values against the 1-1 line, visually you can see how the ECDF points lies below the ideal line (magenta). This means the test is less likely to make false positives but may reduce the tests power.\n\n\nGOAL 3:\n\n\nShow code\nset.seed(1225)\np_values_reduc &lt;- replicate(\n  1000, \n  { \n    #generate responses\n    group1_response_reduc &lt;- rbinom(100, size = 1, prob = 0.20)  #20% long COVID rate\n    group2_response_reduc &lt;- rbinom(100, size = 1, prob = 0.15)  #15% long COVID rate \n              #(25% reducton since 0.20/.75= 0.15)\n    \n    #combine into df\n    fake_data &lt;- data.frame(\n      response = c(group1_response_reduc, group2_response_reduc),\n      group = c(rep(\"control\", 100), rep(\"treatment\", 100))\n    )\n    \n    # run permutation test and extract p-value\n    perm_test_simR &lt;- perm_test(df = fake_data, R = 999, alternative = \"t\") |&gt;\n      purrr::pluck(\"p.val\")\n  }\n)\nalpha &lt;- 0.05\n#estimated power\nmean(p_values_reduc &lt;= alpha)\n\n\n[1] 0.118\n\n\nThe control group had a 20% long COVID rate and the treatment group had a 15% rate. I then ran the permutation test where the p-values were recorded.\nThe proportion of simulations where the test rejected the null (at alpha = 0.05) was 11.8%. This means that the permutation test is unlikely to detect a 25% reduction in risk of long COVID."
  },
  {
    "objectID": "Projects/CompStats/LongCOVID/LongCOVID.html#section-3-sensitivity-analysis",
    "href": "Projects/CompStats/LongCOVID/LongCOVID.html#section-3-sensitivity-analysis",
    "title": "Project 2: Hypothesis Tests",
    "section": "Section 3: Sensitivity Analysis",
    "text": "Section 3: Sensitivity Analysis\n\n\nShow code\n#parameters I could change\nn &lt;- 100\n#if changing sample size instead use to give 10 conditions:\n#sample_sizes &lt;- seq(20, 200, length.out = 10)\nalpha &lt;- 0.05\n\n#changing relative risk\nRR &lt;- 1  # fixed at 1 for now\n\n#control probabilities\np_control_vals &lt;- seq(0.10, 0.25, length.out = 10)\n#prob_control &lt;- 0.20 if changing treatment\n\n#treatment probabilities\n#p_treatment_vals &lt;- seq(0.10, 0.25, length.out = 10) if changing treatment\np_treatment &lt;- 0.20  #fixed otherwise\n\nfisher_test &lt;- function(p_control) { #change to p_treat if changing treatment\n  p_values &lt;- replicate(\n    1000,\n    {\n      group_treat &lt;- rbinom(n, 1, p_treatment)\n      group_ctrl  &lt;- rbinom(n, 1, p_control)\n      \n      tbl &lt;- table(\n        response = c(group_treat, group_ctrl),\n        group    = c(rep(\"treatment\", n), rep(\"control\", n))\n      )\n      fisher.test(tbl)$p.value\n    }\n  )\n  #estimated power\n  mean(p_values &lt;= alpha)\n}\n\n#apply the function to all treatment probabilities\n  #change \"p_control_vals to p_treatment if changing that\npower_estimates &lt;- sapply(p_control_vals, fisher_test)\n  #sapply applies my function to the vectors and outputs a vector (??) of numbers\n\n# Plot with help of chat to understand what the heck im looking at\nplot(p_control_vals, power_estimates, type = \"b\", pch = 16, col = \"magenta\",\n     xlab = \"Control probability of Long COVID\",\n     ylab = \"Estimated power\",\n     main = \"Sensitivity Analysis: Power vs Control Probability\")\nabline(h = alpha, col = \"grey\", lty = 2)\n\n\n\n\n\n\n\n\n\nI performed the sensitivity analysis on varying control group probability to see if it affects the power of the test. I picked 10 control probabilities between 0.10 and 0.25 with the treatment probability fixed at 0.20.\nThe results showed that when the control probabilty is greater or less than 0.20, the estimated power is high (highest at probability 0.10). Meaning it detects the difference between the treatment and control groups more frequently when the control probability isnt 0.20. When the control group probability equal to the control probabilty estimated power was at its lowest.\nHowever, even in these conditions, the estimated power stays low and never reached that common 80% threshold. This may indicate that some other change needs to be made like a larger sample size or bigger differences between control and treatment probabilities."
  },
  {
    "objectID": "Projects/IntDataAnalysis/COVID/COVID.html",
    "href": "Projects/IntDataAnalysis/COVID/COVID.html",
    "title": "Covid Lung Capacity Analysis",
    "section": "",
    "text": "Math 439 HW 1\nNatalie Torres\nNote: Worked with Ashley on the code :)\n\nIntroduction\nWe are analyzing whether vaccinated individuals have higher lung capacity after contracting Covid compared to unvaccinated individuals.\nIn the data given, lung capacity is measured on a scale from 0–100. The control group consists of individuals who contracted Covid and were unvaccinated, while the vaccinated group received the vaccine before getting Covid.\n\n\nData Loading\n\n\nShow code\ncovid &lt;- read.csv(\"Instruction_Data/covid.csv\")\ncontrol &lt;- covid$control\nvaccinated &lt;- covid$vaccinated\n# control: individuals who contracted virus and no vaccine\n# vaccinated: individuals who took vaccine then got Covid\n\n\n\n\nPART A: Two-sample t-test\nWe’re concernced about whether or not vaccinated individuals have a have a higher lung capacity compared to unvaccinated individuals. Let H₀ be the null hypothesis that unvaccinated individuals have the same average lung capacity as vaccinated individuals.\n\n\nShow code\n# parameters\nc.x.bar &lt;- mean(control)\nv.x.bar &lt;- mean(vaccinated)\nobs.diff &lt;- v.x.bar - c.x.bar # needed later for simulation\n\ns.control &lt;- sd(control)\ns.vaccinated &lt;- sd(vaccinated)\n\nn &lt;- length(control)\n\n# t-test (one-sided, greater)\nt.test(vaccinated, control, alternative=\"greater\")\n\n\n\n    Welch Two Sample t-test\n\ndata:  vaccinated and control\nt = 3.4094, df = 398, p-value = 0.0003588\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 4.258564      Inf\nsample estimates:\nmean of x mean of y \n 63.01579  54.76957 \n\n\nThe result of our two-sample t-test shows that the p-value = 0.0003588 (less than 0.05), we reject the null hypothesis that the group averages are equal. Thus, the average lung capacity in the vaccinated group is higher than in the control group.\n\n\nPART B: Bootstrapping\nNot convinced? We can run a simulation 10,000 times to determine if we get the same outcome for more then the just the sample size we have.\n\n\nShow code\nset.seed(42)\n# Histograms\n# hist(control, breaks = 5)\n# hist(vaccinated, breaks = 5)\n\n# Recenter to make H0 true (both means equal)\ncontrol.pseudo.pop &lt;- control - mean(control) + 50\nvaccinated.pseudo.pop &lt;- vaccinated - mean(vaccinated) + 50\n\n# 10000 simulations for control\nc.sim.xbars = rep(0,10000)\nfor(i in 1:10000){\n  c.sim.xbars[i] = mean(sample(control.pseudo.pop,n,replace=T))\n}\n\n# 10000 simulations for vaccinated\nv.sim.xbars = rep(0,10000)\nfor(i in 1:10000){\n  v.sim.xbars[i] = mean(sample(vaccinated.pseudo.pop,n,replace=T))\n}\n# Simulated differences\nsim.diff = v.sim.xbars - c.sim.xbars\n\n# Plot\nhist(sim.diff, breaks = 20,\n     main = \"Histogram of Bootstrap\",\n     xlab = \"Difference in Means (Vaccinated − Control)\")\nlines(c(obs.diff, obs.diff), c(0, 10000), col = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nShow code\n# Simulated p-val\nsim.pval = length(sim.diff[sim.diff&gt;obs.diff])/10000\nsim.pval\n\n\n[1] 3e-04\n\n\nIn this histogram, the red dashed line shows the difference we actually observed. Because it’s way out on the edge we can reject the null hypothesis (H₀).\nThe two-sample t-test and the bootstrap simulation both assess whether the vaccinated group has higher mean lung capacity than the control group. The t-test gave a p-value of 0.00036, while the bootstrap gave a p-value of 0.0003. Both results indicate that vaccinated individuals have a higher lung capacity. Getting the same p-value from both method methods, further solidifies this claim.\n\n\nPART C: Bootstrapping the 10th Percentile\nConsider the idea that we may want to look at the worse cases. So we take the 10th percentile of both groups and compare them.\n\n\nShow code\nset.seed(42)\n# Observed 10th percentile difference\np10.obs.diff &lt;- quantile(vaccinated, 0.10) - quantile(control, 0.10)\n\n# 10000 simulations for control\nc10.sim.xbars = rep(0,10000)\nfor(i in 1:10000){\n  c10.sim.xbars[i] = quantile(sample(control.pseudo.pop,n,replace=T),0.10)\n}\n\n# 10000 simulations for vaccinated\nv10.sim.xbars = rep(0,10000)\nfor(i in 1:10000){\n  v10.sim.xbars[i] = quantile(sample(vaccinated.pseudo.pop,n,replace=T),0.10)\n}\n\n# Simulated difference\np10.sim.diff = v10.sim.xbars - c10.sim.xbars\n\n# Plot\nhist(p10.sim.diff,breaks=20,\n     main = \"Histogram of Bootstrap 10th Percentile\",\n     xlab = \"Difference in Means (Vaccinated − Control)\")\nlines(c(p10.obs.diff,p10.obs.diff),c(0,10000),col=2,lty=2)\n\n\n\n\n\n\n\n\n\nShow code\np10.sim.pval = length(p10.sim.diff[p10.sim.diff&gt;p10.obs.diff])/10000\np10.sim.pval\n\n\n[1] 0.0367\n\n\nSince the p-value is for the 10th percentile is 0.0367 this means that the lowest lung capacities in the vaccinated group are likely higher than those in the control group and is not just random. Meaning even at their worst, vaccinated individuals tend to have better lung capacities than the control group.\n\n\nPart D: Bias Discussion\nIt’s important to acknowledge that Part C can be biased. The sample we have is only a subset of the population, so it could contain more low or high values than the population. This means the 10th percentile we calculated here could overestimate or underestimate the true population 10th percentile.\nUsing the mean can be more reliable, since it uses all values rather than focusing on extremes. If we do want to use percentiles, it’s helpful to report confidence intervals and consider other percentiles as well. That way we can show whether the observed 10th percentile is part of a consistent trend or just due to sampling variability.\n\n\nConclusion\nThe analysis suggests that vaccinated individuals tend to have higher lung capacities than the control group. This indicates that the antibodies developed from vaccination may help maintain healthier lungs even when infected with Covid."
  }
]